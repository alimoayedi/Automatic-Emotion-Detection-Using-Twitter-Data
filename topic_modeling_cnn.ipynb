{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ombtYSSrAFvC",
        "outputId": "fe65d633-b33a-45fb-a7b7-c5a4aec1553c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
          ]
        }
      ],
      "source": [
        "%reset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPlHTYA05qWL"
      },
      "source": [
        "# Importing All Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUA1gXMb5qr9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import utilities as ut\n",
        "import nltk\n",
        "import random\n",
        "\n",
        "from LoadDataset import LoadReutersDataset\n",
        "\n",
        "from nltk import pos_tag\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.svm import LinearSVR\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import cross_validate, cross_val_score, KFold\n",
        "\n",
        "from tensorflow.keras.preprocessing import text, sequence\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Embedding, LSTM, Dense, concatenate, Flatten, Dropout, Conv1D, MaxPooling1D, AveragePooling1D\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import load_model\n",
        "\n",
        "\n",
        "# from google.colab import drive, files\n",
        "\n",
        "\n",
        "# nltk.download('averaged_perceptron_tagger')\n",
        "# nltk.download('wordnet')\n",
        "\n",
        "# # Download GloVe embeddings\n",
        "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "# !unzip glove.6B.zip\n",
        "\n",
        "# drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGSA5l1mN2I7"
      },
      "source": [
        "# Import functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KywX9DLkxRV"
      },
      "outputs": [],
      "source": [
        "# Define the function to remove duplicates from a list\n",
        "def remove_duplicates_terms(doc):\n",
        "    return list(set(doc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CxS5gNKcI5R"
      },
      "outputs": [],
      "source": [
        "# join tokens to make a string\n",
        "def join_tokens(tokens):\n",
        "   return \" \".join(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSgMj0-Tm68n"
      },
      "outputs": [],
      "source": [
        "# replace topics with their indexes, unfound topics are replaced by zero!\n",
        "def replace_with_index(topic_lst):\n",
        "    return [favorite_topics.index(topic) if topic in favorite_topics else 99 for topic in topic_lst]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blZP0XPooUgo"
      },
      "outputs": [],
      "source": [
        "# remove zeros from a list of topics' indexes\n",
        "def remove_zeros(topic_lst):\n",
        "    return [topic for topic in topic_lst if topic != 99]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNtveieJJEe8"
      },
      "outputs": [],
      "source": [
        "def vectorize(vocab_vec, tokens_lst):\n",
        "    return [vocab_vec[term] for term in tokens_lst if term in vocab_vec]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESZVrj9v9rId"
      },
      "outputs": [],
      "source": [
        "def remove_high_correlated_tokens(cosine_sim_score, tokens_lst):\n",
        "    to_remove = set()\n",
        "    for term_1 in tokens_lst:\n",
        "        for term_2 in tokens_lst:\n",
        "            if term_1 != term_2 and cosine_sim_score[term_1, term_2] > 0.9:\n",
        "                    to_remove.add(term_2)\n",
        "    return [term for term in tokens_lst if term not in to_remove]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wX1Cw0x0z9F"
      },
      "outputs": [],
      "source": [
        "def lemmatize(tokenized_text):\n",
        "    # Perform POS tagging\n",
        "    pos_tags = nltk.pos_tag(tokenized_text)\n",
        "\n",
        "    # Initialize WordNetLemmatizer\n",
        "    lemmatizer = nltk.WordNetLemmatizer()\n",
        "\n",
        "    # Lemmatize words using POS tags\n",
        "    lemmas = []\n",
        "    for word, pos_tag in pos_tags:\n",
        "        # Map POS tags to WordNet tags\n",
        "        if pos_tag.startswith('N'):\n",
        "            wn_tag = 0  # Noun\n",
        "        elif pos_tag.startswith('V'):\n",
        "            wn_tag = 1  # Verb\n",
        "        elif pos_tag.startswith('J'):\n",
        "            wn_tag = 2  # Adjective\n",
        "        elif pos_tag.startswith('R'):\n",
        "            wn_tag = 3  # Adverb\n",
        "        else:\n",
        "            wn_tag = 4  # No specific tag\n",
        "\n",
        "        # Lemmatize the word with WordNet\n",
        "        lemmas.append(wn_tag)\n",
        "\n",
        "    return lemmas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3uNqIt5FxQK"
      },
      "outputs": [],
      "source": [
        "def white_space_splitter(text):\n",
        "    return text.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIOzCiXhEcnC"
      },
      "outputs": [],
      "source": [
        "def padding(lst, maximum_length):\n",
        "    return sequence.pad_sequences([lst], maxlen=maximum_length, padding='post')[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yi7KdwhrOIMS"
      },
      "outputs": [],
      "source": [
        "def make_tuple(lst, size):\n",
        "    list_size = len(lst)\n",
        "    if size == 2:\n",
        "        return [(lst[index], lst[index+1]) for index in range(list_size - size + 1)]\n",
        "    elif size == 3:\n",
        "        return [(lst[index], lst[index+1], lst[index+2]) for index in range(list_size - size + 1)]\n",
        "    else:\n",
        "        raise ValueError(\"Error in the value of the size! Check the method!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rEOrvH5DAb8"
      },
      "outputs": [],
      "source": [
        "def get_unique_token_pairs(lst_docs):\n",
        "    unique_pairs = []\n",
        "    for doc in lst_docs:\n",
        "        unique_pairs.extend(list(set(doc)))\n",
        "    return list(set(unique_pairs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lrOtaakuPNW"
      },
      "outputs": [],
      "source": [
        "def get_token_pairs_count(token_pair, lst):\n",
        "    if token_pair in lst:\n",
        "        return lst.count(token_pair)\n",
        "    else:\n",
        "        return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4znn6QgU65D"
      },
      "outputs": [],
      "source": [
        "# create a list of elements from columns of DF\n",
        "def create_list(row):\n",
        "    elements = row.iloc[:].tolist()\n",
        "    return elements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8X5y-2Pd5LZO"
      },
      "outputs": [],
      "source": [
        "\n",
        "def feature_selection_rfe(X_train, y_train):\n",
        "    # Create the SVR estimator\n",
        "    svr_estimator = LinearSVR(max_iter = 10000)\n",
        "\n",
        "    # Create the RFE object with desired estimator and number of features to select\n",
        "    selector = RFE(estimator=svr_estimator, n_features_to_select=0.1, step=100)\n",
        "\n",
        "    # Fit the RFE on the training data\n",
        "    selector.fit(X_train, y_train)\n",
        "\n",
        "    # Extract the selected features based on RFE rankings\n",
        "    feature_ranks = selector.ranking_\n",
        "\n",
        "    # Get the selected features based on model performance\n",
        "    selected_features = X_train.columns[selector.support_]\n",
        "\n",
        "    return selected_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuZ-6RvOLfNV"
      },
      "outputs": [],
      "source": [
        "def get_number_of_tokens(df_col):\n",
        "    all_items = [item for sublist in df_col for item in sublist]\n",
        "\n",
        "    # Step 2: Convert to a set to find unique items\n",
        "    unique_items = set(all_items)\n",
        "\n",
        "    # Step 3: Count the number of unique items\n",
        "    return len(unique_items)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMPu3AEWzR3c"
      },
      "outputs": [],
      "source": [
        "def concatenate_arrays(columns, row):\n",
        "    concatinated = np.array([])\n",
        "    for col in columns:\n",
        "        concatinated = np.append(concatinated, row[col])\n",
        "    return concatinated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26bqEovWGjS8"
      },
      "outputs": [],
      "source": [
        "def GloVe_embedding(word_index_dict, vocab_size, embedding_dim):\n",
        "\n",
        "    embedding_dim = 100\n",
        "    embedding_file = 'glove.6B.100d.txt'\n",
        "    embeddings_index = {}\n",
        "\n",
        "    with open(embedding_file) as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "    for word, index in word_index_dict.items():\n",
        "        if index < vocab_size:\n",
        "            embedding_vector = embeddings_index.get(word)\n",
        "            if embedding_vector is not None:\n",
        "                embedding_matrix[index] = embedding_vector\n",
        "\n",
        "    return embedding_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuwZdtYm4ffq"
      },
      "source": [
        "# **Import Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0sbrqiZyqre"
      },
      "source": [
        "## Import ag_news"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "0ESVvlvhyqGW",
        "outputId": "83994d73-ff43-4ede-e509-2ce02f374f4b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/train.parquet'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-07d1fb6bb419>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdataset_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'train.parquet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'test.parquet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'doc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, **kwargs)\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[0mcheck_dtype_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype_backend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m     return impl.read(\n\u001b[0m\u001b[1;32m    510\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, path, columns, use_nullable_dtypes, dtype_backend, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mto_pandas_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"split_blocks\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# type: ignore[assignment]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         path_or_handle, handles, kwargs[\"filesystem\"] = _get_path_or_handle(\n\u001b[0m\u001b[1;32m    221\u001b[0m             \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"filesystem\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;31m# fsspec resources can also point to directories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;31m# this branch is used for example when reading from non-fsspec URLs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         handles = get_handle(\n\u001b[0m\u001b[1;32m    111\u001b[0m             \u001b[0mpath_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    869\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/train.parquet'"
          ]
        }
      ],
      "source": [
        "dataset_path = '/content/'\n",
        "train = pd.read_parquet(dataset_path + 'train.parquet')\n",
        "test = pd.read_parquet(dataset_path + 'test.parquet')\n",
        "\n",
        "train.columns = ['doc', 'label']\n",
        "test.columns = ['doc', 'label']\n",
        "train.index.name = 'index'\n",
        "test.index.name = 'index'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JY7Zxi0wP8V_"
      },
      "outputs": [],
      "source": [
        "documents = pd.DataFrame(train['doc'], index=train.index, columns=['doc'])\n",
        "topics = pd.DataFrame(train['label'], index=train.index, columns=['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NK77MgjTiDPX"
      },
      "outputs": [],
      "source": [
        "test_documents = pd.DataFrame(test['doc'], index=test.index, columns=['doc'])\n",
        "test_topics = pd.DataFrame(test['label'], index=test.index, columns=['label'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyDYe-MKyrOA"
      },
      "source": [
        "## Import reuters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FQTRUx55bqF",
        "outputId": "5ebfca43-80e6-48c7-a482-b5829472df16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** opening file:  /content/drive/MyDrive/ColabNotebooks/reuters21578/reut2-000.sgm\n",
            "*** opening file:  /content/drive/MyDrive/ColabNotebooks/reuters21578/reut2-001.sgm\n",
            "*** opening file:  /content/drive/MyDrive/ColabNotebooks/reuters21578/reut2-002.sgm\n",
            "*** opening file:  /content/drive/MyDrive/ColabNotebooks/reuters21578/reut2-003.sgm\n",
            "*** opening file:  /content/drive/MyDrive/ColabNotebooks/reuters21578/reut2-004.sgm\n",
            "*** opening file:  /content/drive/MyDrive/ColabNotebooks/reuters21578/reut2-006.sgm\n",
            "*** opening file:  /content/drive/MyDrive/ColabNotebooks/reuters21578/reut2-005.sgm\n",
            "*** opening file:  /content/drive/MyDrive/ColabNotebooks/reuters21578/reut2-007.sgm\n",
            "*** opening file:  /content/drive/MyDrive/ColabNotebooks/reuters21578/reut2-008.sgm\n",
            "*** opening file:  /content/drive/MyDrive/ColabNotebooks/reuters21578/reut2-009.sgm\n",
            "*** opening file:  /content/drive/MyDrive/ColabNotebooks/reuters21578/reut2-011.sgm\n",
            "*** opening file:  /content/drive/MyDrive/ColabNotebooks/reuters21578/reut2-010.sgm\n",
            "*** opening file:  /content/drive/MyDrive/ColabNotebooks/reuters21578/reut2-012.sgm\n",
            "*** opening file:  /content/drive/MyDrive/ColabNotebooks/reuters21578/reut2-013.sgm\n",
            "*** opening file:  /content/drive/MyDrive/ColabNotebooks/reuters21578/reut2-014.sgm\n",
            "*** opening file:  /content/drive/MyDrive/ColabNotebooks/reuters21578/reut2-015.sgm\n",
            "*** opening file:  /content/drive/MyDrive/ColabNotebooks/reuters21578/reut2-016.sgm\n",
            "*** opening file:  /content/drive/MyDrive/ColabNotebooks/reuters21578/reut2-017.sgm\n",
            "*** opening file:  /content/drive/MyDrive/ColabNotebooks/reuters21578/reut2-018.sgm\n",
            "*** opening file:  /content/drive/MyDrive/ColabNotebooks/reuters21578/reut2-019.sgm\n",
            "*** opening file:  /content/drive/MyDrive/ColabNotebooks/reuters21578/reut2-020.sgm\n",
            "*** opening file:  /content/drive/MyDrive/ColabNotebooks/reuters21578/reut2-021.sgm\n"
          ]
        }
      ],
      "source": [
        "dataset_path = '/content/drive/MyDrive/ColabNotebooks'\n",
        "loader = LoadReutersDataset(data_path=dataset_path + '/reuters21578')\n",
        "documents_dic, topics_dic, _, _, _, _, _ = loader.load()\n",
        "\n",
        "documents = pd.DataFrame.from_dict(documents_dic, orient='index', columns=['doc'])\n",
        "topics = pd.DataFrame.from_dict(topics_dic, orient='index')\n",
        "\n",
        "# # If you want to name the index, you can set the index name\n",
        "documents.index.name = 'index'\n",
        "topics.index.name = 'index'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmcFLPx8U_0e"
      },
      "source": [
        "# **Filter Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHQQyld2U_bY"
      },
      "outputs": [],
      "source": [
        "# remove all the documents without any specific topic\n",
        "documents = documents[topics.notna().any(axis=1)]\n",
        "topics = topics[topics.notna().any(axis=1)]\n",
        "\n",
        "# filter documents and keep only the ones with favorite topics\n",
        "# favorite_topics = ['acq', 'money-fx', 'grain', 'crude', 'trade', 'interest', 'ship', 'wheat', 'corn', 'oilseed']\n",
        "favorite_topics = ['acq', 'corn', 'crude', 'earn']\n",
        "documents = documents[topics.isin(favorite_topics).any(axis=1)]\n",
        "topics = topics[topics.isin(favorite_topics).any(axis=1)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBn_8r789YT3"
      },
      "source": [
        "# **Sampling**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2yMG3VO9XUK"
      },
      "outputs": [],
      "source": [
        "rand_sample = 2000\n",
        "documents = pd.DataFrame(documents.sample(n=rand_sample, random_state=42, replace=False))\n",
        "topics = pd.DataFrame(topics.loc[documents.index])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFFE6HaN2Zol"
      },
      "outputs": [],
      "source": [
        "# num_samples = 2000\n",
        "# topics_count = []\n",
        "# for topic in favorite_topics:\n",
        "#     related_topic_doc_index = list[topics.index[topics.applymap(lambda x: x == 'acq').any(axis=1)]]\n",
        "#     num_samples * len(related_topic_doc_index) / len(topics)\n",
        "#     topics_count.append()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAiG1vSQ-GlM"
      },
      "source": [
        "# Save Random Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ew-Z7lF-F_2"
      },
      "outputs": [],
      "source": [
        "# documents.to_hdf('/content/drive/My Drive/4topics_experiment.h5', key='documents', mode='w')\n",
        "# topics.to_hdf('/content/drive/My Drive/4topics_experiment.h5', key='topics', mode='a')\n",
        "\n",
        "# documents.to_csv('/content/drive/My Drive/sampled_documents.csv', index=True)\n",
        "# topics.to_csv('/content/drive/My Drive/sampled_topics.csv', index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCxMcty3_XKr"
      },
      "source": [
        "# Load Sampled Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ei1wLf_w_W9x"
      },
      "outputs": [],
      "source": [
        "documents = pd.read_hdf('/content/drive/My Drive/4topics_experiment.h5', key='documents')\n",
        "topics = pd.read_hdf('/content/drive/My Drive/4topics_experiment.h5', key='topics')\n",
        "\n",
        "\n",
        "# documents = pd.read_csv('/content/drive/My Drive/sampled_documents.csv')\n",
        "# documents.set_index('index', inplace=True)\n",
        "\n",
        "# topics = pd.read_csv('/content/drive/My Drive/sampled_topics.csv')\n",
        "# topics.set_index('index', inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEl0CPCM2WdU"
      },
      "source": [
        "# **Filter Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kw8obdfhLNXt"
      },
      "source": [
        "# Pre-process ag_news"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "nlzlWS5sLOHD",
        "outputId": "add4bde0-8056-4f6b-914a-8038e36b147b"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-913a5c9c3be5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdocuments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'preprocess'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'doc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# drop preprocessed documents with length less than 6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtopics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'preprocess'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'preprocess'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4628\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4629\u001b[0m         \"\"\"\n\u001b[0;32m-> 4630\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4632\u001b[0m     def _reduce(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m         \u001b[0;31m# self.f is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1025\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1074\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1076\u001b[0;31m                 mapped = lib.map_infer(\n\u001b[0m\u001b[1;32m   1077\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/content/utilities.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiltered_tokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcachedStopWords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;31m# stemmed_tokens = (PorterStemmer().stem(token) for token in filtered_tokens)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m     \u001b[0mlemmatized_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_wordnet_pos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlemmatized_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/utilities.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiltered_tokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcachedStopWords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;31m# stemmed_tokens = (PorterStemmer().stem(token) for token in filtered_tokens)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m     \u001b[0mlemmatized_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_wordnet_pos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlemmatized_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/utilities.py\u001b[0m in \u001b[0;36mget_wordnet_pos\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_wordnet_pos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m     \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m     tag_dict = {\n\u001b[1;32m    146\u001b[0m         \u001b[0;34m\"J\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mADJ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \"\"\"\n\u001b[1;32m    165\u001b[0m     \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36m_pos_tag\u001b[0;34m(tokens, tagset, tagger, lang)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mtagged_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Maps to the specified tagset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlang\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"eng\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36mtag\u001b[0;34m(self, tokens, return_conf, use_tagdict)\u001b[0m\n\u001b[1;32m    184\u001b[0m             )\n\u001b[1;32m    185\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m                 \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m                 \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_conf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_conf\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36m_get_features\u001b[0;34m(self, i, word, context, prev, prev2)\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTART\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;31m# It's useful to have a constant feature, which acts sort of like a prior\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "documents['preprocess'] = documents['doc'].apply(ut.tokenize)\n",
        "\n",
        "# drop preprocessed documents with length less than 6\n",
        "topics = topics[documents['preprocess'].str.len() > 6]\n",
        "documents = documents[documents['preprocess'].str.len() > 6]\n",
        "\n",
        "favorite_topics = [0, 1, 2, 3]\n",
        "\n",
        "# remove duplicate terms from each document\n",
        "# documents['preprocess'] = documents['preprocess'].apply(remove_duplicates_terms)\n",
        "\n",
        "#join preprocced tokens to make a string. used in tf-idf and cosine scoring.\n",
        "documents['joined_tokens'] = documents['preprocess'].apply(join_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lM5kHZiLhtYe"
      },
      "outputs": [],
      "source": [
        "test_documents['preprocess'] = test_documents['doc'].apply(ut.tokenize)\n",
        "test_documents['joined_tokens'] = test_documents['preprocess'].apply(join_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dp_AzMU4LOkl"
      },
      "source": [
        "# Pre-process reuters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JT9pbguLxUYF",
        "outputId": "d8cd7ca9-2b93-4429-adab-7049cb823252"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-102-59ffd3166764>:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  documents['joined_tokens'] = documents['preprocess'].apply(join_tokens)\n"
          ]
        }
      ],
      "source": [
        "# preprocess data by tokenization\n",
        "documents['preprocess'] = documents['doc'].apply(ut.tokenize)\n",
        "\n",
        "# drop preprocessed documents with length less than 6\n",
        "topics = topics[documents['preprocess'].str.len() > 6]\n",
        "documents = documents[documents['preprocess'].str.len() > 6]\n",
        "\n",
        "# remove duplicate terms from each document\n",
        "# documents['preprocess'] = documents['preprocess'].apply(remove_duplicates_terms)\n",
        "\n",
        "#join preprocced tokens to make a string. used in tf-idf and cosine scoring.\n",
        "documents['joined_tokens'] = documents['preprocess'].apply(join_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "757dPthIOnqu"
      },
      "source": [
        "# Hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gC-v5FSOm5U"
      },
      "outputs": [],
      "source": [
        "maximum_length = 100\n",
        "embedding_dim = 100\n",
        "num_extra_features = 100\n",
        "num_classes = len(favorite_topics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALHuYuZN14eM"
      },
      "source": [
        "# Coding Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22BOj46ilnNK",
        "outputId": "fbf97a07-911c-4174-b191-3b02940506ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-104-6752a340b29e>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  topics['topics_lst'] = topics.iloc[:, :].apply(lambda row: list(row), axis=1)\n"
          ]
        }
      ],
      "source": [
        "# combine all the topics into a list\n",
        "topics['topics_lst'] = topics.iloc[:, :].apply(lambda row: list(row), axis=1)\n",
        "\n",
        "# replace topics with their indexes, unfound topics are replaced by zero\n",
        "topics['topics_lst'] = topics['topics_lst'].apply(replace_with_index)\n",
        "\n",
        "# Apply the function to column 'Column'\n",
        "topics['topics_lst'] = topics['topics_lst'].apply(remove_zeros)\n",
        "\n",
        "# convert labels into a one-hot coding\n",
        "topics['one_hot'] = [list(np.sum(to_categorical(label, num_classes=num_classes), axis=0)) for label in topics['topics_lst']]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-GKyjGnjbaF"
      },
      "source": [
        "# Coding Labels (AG_NEWS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gg0gh4A-i6aF"
      },
      "outputs": [],
      "source": [
        "# combine all the topics into a list\n",
        "test_topics['topics_lst'] = test_topics.iloc[:, :].apply(lambda row: list(row), axis=1)\n",
        "\n",
        "# replace topics with their indexes, unfound topics are replaced by zero\n",
        "test_topics['topics_lst'] = test_topics['topics_lst'].apply(replace_with_index)\n",
        "\n",
        "# Apply the function to column 'Column'\n",
        "test_topics['topics_lst'] = test_topics['topics_lst'].apply(remove_zeros)\n",
        "\n",
        "# convert labels into a one-hot coding\n",
        "test_topics['one_hot'] = [list(np.sum(to_categorical(label, num_classes=num_classes), axis=0)) for label in test_topics['topics_lst']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCGH0axcBbVo"
      },
      "source": [
        "# **Split Data into Train and Validation, and Test set**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuGxOq79joCs"
      },
      "source": [
        "# AG_NEWS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "0Xh-us1qjl4v",
        "outputId": "774088a3-0439-4b4d-f723-99198245a888"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'test_documents' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-34255a31215c>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrainDocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalDocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainTopics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalTopics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtestDocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_documents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtestTopics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_topics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainDocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test_documents' is not defined"
          ]
        }
      ],
      "source": [
        "rand = random.randint(10,99)\n",
        "\n",
        "trainDocs, valDocs, trainTopics, valTopics = train_test_split(documents, topics, test_size=0.2, random_state=rand)\n",
        "testDocs = test_documents\n",
        "testTopics = test_topics\n",
        "print(trainDocs.shape)\n",
        "print(trainTopics.shape)\n",
        "print(valDocs.shape)\n",
        "print(valTopics.shape)\n",
        "print(testDocs.shape)\n",
        "print(testTopics.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iklnNbA8jomV"
      },
      "source": [
        "# Reuters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iO5CpSW21TYa",
        "outputId": "643041ee-77c4-44a4-f574-e1e618d3811a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1279, 3)\n",
            "(1279, 18)\n",
            "(320, 3)\n",
            "(320, 18)\n",
            "(400, 3)\n",
            "(400, 18)\n",
            "\n",
            "_______________________\n",
            "\n",
            "Category  acq  counts in the train set: 406\n",
            "Category  acq  counts in the validation set: 108\n",
            "Category  acq  counts in the test set: 116\n",
            "_______________________\n",
            "Category  corn  counts in the train set: 41\n",
            "Category  corn  counts in the validation set: 16\n",
            "Category  corn  counts in the test set: 13\n",
            "_______________________\n",
            "Category  crude  counts in the train set: 124\n",
            "Category  crude  counts in the validation set: 22\n",
            "Category  crude  counts in the test set: 36\n",
            "_______________________\n",
            "Category  earn  counts in the train set: 725\n",
            "Category  earn  counts in the validation set: 176\n",
            "Category  earn  counts in the test set: 241\n",
            "_______________________\n"
          ]
        }
      ],
      "source": [
        "rand = random.randint(10,99)\n",
        "\n",
        "trainValDocs, testDocs, trainValTopics, testTopics = train_test_split(documents, topics, test_size=0.2, random_state=rand)\n",
        "trainDocs, valDocs, trainTopics, valTopcis = train_test_split(trainValDocs, trainValTopics, test_size=0.2, random_state=rand)\n",
        "print(trainDocs.shape)\n",
        "print(trainTopics.shape)\n",
        "print(valDocs.shape)\n",
        "print(valTopcis.shape)\n",
        "print(testDocs.shape)\n",
        "print(testTopics.shape)\n",
        "print(\"\\n_______________________\\n\")\n",
        "# Print the count of documents in each category for train, validation, and test sets\n",
        "for topic in favorite_topics:\n",
        "    length = len(trainTopics.index[trainTopics.applymap(lambda x: x == topic).any(axis=1)])\n",
        "    print(\"Category \", topic, \" counts in the train set:\", length)\n",
        "    length = len(valTopcis.index[valTopcis.applymap(lambda x: x == topic).any(axis=1)])\n",
        "    print(\"Category \", topic, \" counts in the validation set:\", length)\n",
        "    length = len(testTopics.index[testTopics.applymap(lambda x: x == topic).any(axis=1)])\n",
        "    print(\"Category \", topic, \" counts in the test set:\", length)\n",
        "    print(\"_______________________\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2plhochKzbC"
      },
      "source": [
        "# **Save Split Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Heu4ieu5KzEd"
      },
      "outputs": [],
      "source": [
        "# trainDocs.to_hdf('/content/drive/My Drive/4topics_experiment.h5', key='trainDocs', mode='a')\n",
        "# trainTopics.to_hdf('/content/drive/My Drive/4topics_experiment.h5', key='trainTopics', mode='a')\n",
        "# valDocs.to_hdf('/content/drive/My Drive/4topics_experiment.h5', key='valDocs', mode='a')\n",
        "# valTopcis.to_hdf('/content/drive/My Drive/4topics_experiment.h5', key='valTopcis', mode='a')\n",
        "# testDocs.to_hdf('/content/drive/My Drive/4topics_experiment.h5', key='testDocs', mode='a')\n",
        "# testTopics.to_hdf('/content/drive/My Drive/4topics_experiment.h5', key='testTopics', mode='a')\n",
        "\n",
        "# trainDocs.to_csv('/content/drive/My Drive/trainDocs.csv', index=True)\n",
        "# trainTopics.to_csv('/content/drive/My Drive/trainTopics.csv', index=True)\n",
        "# valDocs.to_csv('/content/drive/My Drive/valDocs.csv', index=True)\n",
        "# valTopcis.to_csv('/content/drive/My Drive/valTopcis.csv', index=True)\n",
        "# testDocs.to_csv('/content/drive/My Drive/testDocs.csv', index=True)\n",
        "# testTopics.to_csv('/content/drive/My Drive/testTopics.csv', index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_S5rj6lZLeVQ"
      },
      "source": [
        "# **Load Split Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdd4xbfNLczb",
        "outputId": "9c5099f2-6629-4ca7-8dc7-ecc4dffd064b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1279, 3)\n",
            "(1279, 18)\n",
            "(320, 3)\n",
            "(320, 18)\n",
            "(400, 3)\n",
            "(400, 18)\n",
            "\n",
            "_______________________\n",
            "\n",
            "Category  acq  counts in the train set: 402\n",
            "Category  acq  counts in the validation set: 100\n",
            "Category  acq  counts in the test set: 128\n",
            "_______________________\n",
            "Category  corn  counts in the train set: 46\n",
            "Category  corn  counts in the validation set: 11\n",
            "Category  corn  counts in the test set: 13\n",
            "_______________________\n",
            "Category  crude  counts in the train set: 113\n",
            "Category  crude  counts in the validation set: 31\n",
            "Category  crude  counts in the test set: 38\n",
            "_______________________\n",
            "Category  earn  counts in the train set: 730\n",
            "Category  earn  counts in the validation set: 182\n",
            "Category  earn  counts in the test set: 230\n",
            "_______________________\n"
          ]
        }
      ],
      "source": [
        "trainDocs = pd.read_hdf('/content/drive/My Drive/4topics_experiment.h5', key='trainDocs')\n",
        "trainTopics = pd.read_hdf('/content/drive/My Drive/4topics_experiment.h5', key='trainTopics')\n",
        "valDocs = pd.read_hdf('/content/drive/My Drive/4topics_experiment.h5', key='valDocs')\n",
        "valTopcis = pd.read_hdf('/content/drive/My Drive/4topics_experiment.h5', key='valTopcis')\n",
        "testDocs = pd.read_hdf('/content/drive/My Drive/4topics_experiment.h5', key='testDocs')\n",
        "testTopics = pd.read_hdf('/content/drive/My Drive/4topics_experiment.h5', key='testTopics')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# trainDocs = pd.read_csv('/content/drive/My Drive/trainDocs.csv')\n",
        "# valDocs = pd.read_csv('/content/drive/My Drive/valDocs.csv')\n",
        "# testDocs = pd.read_csv('/content/drive/My Drive/testDocs.csv')\n",
        "# trainTopics = pd.read_csv('/content/drive/My Drive/trainTopics.csv')\n",
        "# valTopcis = pd.read_csv('/content/drive/My Drive/valTopcis.csv')\n",
        "# testTopics = pd.read_csv('/content/drive/My Drive/testTopics.csv')\n",
        "\n",
        "# trainDocs.set_index('index', inplace=True)\n",
        "# valDocs.set_index('index', inplace=True)\n",
        "# testDocs.set_index('index', inplace=True)\n",
        "# trainTopics.set_index('index', inplace=True)\n",
        "# valTopcis.set_index('index', inplace=True)\n",
        "# testTopics.set_index('index', inplace=True)\n",
        "\n",
        "# trainDocs.index = trainDocs.index.astype(str)\n",
        "# valDocs.index = valDocs.index.astype(str)\n",
        "# testDocs.index = testDocs.index.astype(str)\n",
        "# trainTopics.index = trainTopics.index.astype(str)\n",
        "# valTopcis.index = valTopcis.index.astype(str)\n",
        "# testTopics.index = testTopics.index.astype(str)\n",
        "\n",
        "# trainDocs['documents'] = trainDocs['documents'].apply(ast.literal_eval)\n",
        "# valDocs['documents'] = valDocs['documents'].apply(ast.literal_eval)\n",
        "# testDocs['documents'] = testDocs['documents'].apply(ast.literal_eval)\n",
        "# trainTopics['documents'] = trainTopics['documents'].apply(ast.literal_eval)\n",
        "# valTopcis['documents'] = valTopcis['documents'].apply(ast.literal_eval)\n",
        "# testTopics['documents'] = testTopics['documents'].apply(ast.literal_eval)\n",
        "\n",
        "\n",
        "print(trainDocs.shape)\n",
        "print(trainTopics.shape)\n",
        "print(valDocs.shape)\n",
        "print(valTopcis.shape)\n",
        "print(testDocs.shape)\n",
        "print(testTopics.shape)\n",
        "print(\"\\n_______________________\\n\")\n",
        "# Print the count of documents in each category for train, validation, and test sets\n",
        "for topic in favorite_topics:\n",
        "    length = len(trainTopics.index[trainTopics.applymap(lambda x: x == topic).any(axis=1)])\n",
        "    print(\"Category \", topic, \" counts in the train set:\", length)\n",
        "    length = len(valTopcis.index[valTopcis.applymap(lambda x: x == topic).any(axis=1)])\n",
        "    print(\"Category \", topic, \" counts in the validation set:\", length)\n",
        "    length = len(testTopics.index[testTopics.applymap(lambda x: x == topic).any(axis=1)])\n",
        "    print(\"Category \", topic, \" counts in the test set:\", length)\n",
        "    print(\"_______________________\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLbh5gnGVYg7"
      },
      "source": [
        "# **Cosine Similarity Calculation And Vectorization**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X17YFMDqlneu"
      },
      "source": [
        "**Train Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3mNuRxDYD5A",
        "outputId": "c80cd969-19d8-427e-b71f-66a69c3df5e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(tokenizer=white_space_splitter, preprocessor=None, stop_words=None, max_df=1.0, min_df=1, max_features=None)\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(trainDocs['joined_tokens'])\n",
        "\n",
        "count_vectorizer = CountVectorizer(tokenizer=white_space_splitter, preprocessor=None, stop_words=None, max_df=1.0, min_df=1, max_features=None)\n",
        "tf_matrix = count_vectorizer.fit_transform(trainDocs['joined_tokens'])\n",
        "\n",
        "vocab_size = len(count_vectorizer.vocabulary_)\n",
        "\n",
        "# cosine_sim_score = cosine_similarity(tf_matrix.T)\n",
        "\n",
        "trainDocs['vectorized'] = trainDocs['preprocess'].apply(lambda lst: vectorize(tfidf_vectorizer.vocabulary_, lst))\n",
        "# trainDocs['vectorized'] = trainDocs['vectorized'].apply(lambda lst: remove_high_correlated_tokens(cosine_sim_score, lst))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CT2t1ppSEme6",
        "outputId": "9fa0622f-241e-4633-f301-43e3153d682e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens before cosine similarity: 7503\n",
            "Number of tokens after cosine similarity: 7503\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of tokens before cosine similarity: \" + str(vocab_size))\n",
        "print(\"Number of tokens after cosine similarity: \" + str(get_number_of_tokens(trainDocs['vectorized'])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQzRFS94lqoO"
      },
      "source": [
        "**Vectorize Validation Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ipbBQ1PlrOf"
      },
      "outputs": [],
      "source": [
        "valDocs['vectorized'] = valDocs['preprocess'].apply(lambda lst: vectorize(tfidf_vectorizer.vocabulary_, lst))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrWRE9vL3HJS"
      },
      "source": [
        "# **TF Feature**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiEoGzKHEThM"
      },
      "source": [
        "**Train**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9FVh5sDVyKG"
      },
      "outputs": [],
      "source": [
        "unique_terms = [token for tokenized_doc in trainDocs['vectorized'] for token in tokenized_doc]\n",
        "unique_terms = list(set(unique_terms))\n",
        "train_tf = pd.DataFrame(tf_matrix[:, unique_terms].toarray(), columns=unique_terms, index=trainDocs.index)\n",
        "trainDocs['tf'] = pd.DataFrame(train_tf.apply(create_list, axis=1)).iloc[:, 0].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJF5kY9ESRJo"
      },
      "source": [
        "**Validation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGE74GBzETGx"
      },
      "outputs": [],
      "source": [
        "val_tf_matrix = count_vectorizer.transform(valDocs['joined_tokens'])\n",
        "val_tf = pd.DataFrame(val_tf_matrix[:, unique_terms].toarray(), columns=unique_terms, index=valDocs.index)\n",
        "valDocs['tf'] = pd.DataFrame(val_tf.apply(create_list, axis=1)).iloc[:, 0].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGtzwM1N9lsf"
      },
      "source": [
        "\n",
        "# **Terms Dictionary**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3a1X6Lku27u"
      },
      "source": [
        "**Term Topic Dictionary**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0G2OL7i09lUu"
      },
      "outputs": [],
      "source": [
        "vocab_size = get_number_of_tokens(trainDocs['vectorized'])\n",
        "token_topic_dict = pd.DataFrame(np.zeros(shape=(vocab_size, len(favorite_topics))), columns=range(len(favorite_topics)), index=unique_terms)\n",
        "\n",
        "for index in trainTopics.index:\n",
        "    topics_lst = trainTopics.loc[index]['topics_lst']\n",
        "    term_vector = trainDocs.loc[index]['vectorized']\n",
        "    for topic in topics_lst:\n",
        "        for term in term_vector:\n",
        "            token_topic_dict.loc[term][topic] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkH8TXkfyXQ3",
        "outputId": "7dfd0afb-2eeb-405e-c527-cfc5ca6808e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filter Dictionary?N\n"
          ]
        }
      ],
      "source": [
        "filter_dictionary = input(\"Filter Dictionary?\")\n",
        "if filter_dictionary in [\"Y\", \"y\"]:\n",
        "    for topic in token_topic_dict.columns:\n",
        "        col_of_topic = token_topic_dict[topic] != 0\n",
        "        other_cols = token_topic_dict.drop(columns=topic).eq(0).all(axis=1)\n",
        "\n",
        "        # Combine the conditions\n",
        "        condition = col_of_topic & other_cols\n",
        "\n",
        "        # Mark the rows that meet the condition with 1\n",
        "        token_topic_dict['mark'] = np.where(condition, 1, 0)\n",
        "        topic_terms_index = token_topic_dict.sort_values(by=['mark',topic], ascending=False).iloc[100:].index\n",
        "        token_topic_dict.loc[topic_terms_index, topic] = 0\n",
        "    token_topic_dict.drop(columns='mark', inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOIG80vZuykA"
      },
      "source": [
        "**Train Term Topic Weights**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgYHC8kGx-4M",
        "outputId": "0be8094a-0d16-4afd-9fc9-9fd980801648"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1279, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ],
      "source": [
        "trainTTW = pd.DataFrame(None, columns=range(len(favorite_topics)), index=trainDocs.index)\n",
        "\n",
        "for topic in list(token_topic_dict.columns):\n",
        "    trainTTW[topic] = trainDocs['vectorized'].apply(lambda lst: [token_topic_dict.loc[term][topic] for term in lst])\n",
        "trainTTW.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzI3r71tvS3q"
      },
      "source": [
        "**Padding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WB63UIAHMTtU",
        "outputId": "71482af6-7abe-4171-9631-7edceea60d83"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1279, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ],
      "source": [
        "for col in list(trainTTW.columns):\n",
        "    trainTTW[col] = trainTTW[col].apply(lambda lst: padding(lst, maximum_length))\n",
        "trainTTW.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCMAG2ibwzIs"
      },
      "source": [
        "**Concatinate (Merge) Term Topic Weights**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7-PN5pURGVh"
      },
      "outputs": [],
      "source": [
        "# Apply the function and create a new column\n",
        "columns = trainTTW.columns\n",
        "trainTTW['concatinated'] = trainTTW.apply(lambda lst: concatenate_arrays(columns, lst),  axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQRPvDwrmQ-6"
      },
      "source": [
        "**Validation Term Topic Weights**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dohpVErmQsa",
        "outputId": "4dd9349d-7238-473b-810f-9ee4fe8118ca"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(320, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ],
      "source": [
        "valTTW = pd.DataFrame(None, columns=range(len(favorite_topics)), index=valDocs.index)\n",
        "\n",
        "for topic in list(token_topic_dict.columns):\n",
        "    valTTW[topic] = valDocs['vectorized'].apply(lambda lst: [token_topic_dict.loc[term][topic] if term in token_topic_dict.index else 0 for term in lst])\n",
        "valTTW.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7v5g0KOp30L"
      },
      "source": [
        "**Padding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPjkweoap3e7",
        "outputId": "ed0a241c-2ecf-49c5-ca99-e4893fc8448e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(320, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ],
      "source": [
        "for col in list(valTTW.columns):\n",
        "    valTTW[col] = valTTW[col].apply(lambda lst: padding(lst, maximum_length))\n",
        "valTTW.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MjQgWzLqE9a"
      },
      "source": [
        "**Concatinate (Merge) Term Topic Weights**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhmvT8VVqEsf"
      },
      "outputs": [],
      "source": [
        "# Apply the function and create a new column\n",
        "columns = valTTW.columns\n",
        "valTTW['concatinated'] = valTTW.apply(lambda lst: concatenate_arrays(columns, lst),  axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gRGdnG66yn3"
      },
      "source": [
        "# **TF-iDF**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoGdDlzF6-Z1"
      },
      "source": [
        "**Train**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQhADzK56yQH"
      },
      "outputs": [],
      "source": [
        "# unique terms calculated in the TF part\n",
        "train_tfidf = pd.DataFrame(tfidf_matrix[:, unique_terms].toarray(), columns=unique_terms, index=trainDocs.index)\n",
        "trainDocs['tfidf'] = pd.DataFrame(train_tfidf.apply(create_list, axis=1)).iloc[:, 0].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnonNS_OYR6L"
      },
      "source": [
        "**Test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjYtAaLBYRnE"
      },
      "outputs": [],
      "source": [
        "val_tfidf_matrix = tfidf_vectorizer.transform(valDocs['joined_tokens'])\n",
        "val_tfidf = pd.DataFrame(val_tfidf_matrix[:, unique_terms].toarray(), columns=unique_terms, index=valDocs.index)\n",
        "valDocs['tfidf'] = pd.DataFrame(val_tfidf.apply(create_list, axis=1)).iloc[:, 0].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecwBbTNOSpU4"
      },
      "source": [
        "\n",
        "# **PCA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "Wf9NEs1YSqTg",
        "outputId": "737c52b1-e84d-48ef-9119-07f784175e94"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "setting an array element with a sequence.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a real number, not 'list'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-eab0d8cfb37c>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mX_train_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainDocs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vectorized'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# X_test_scaled = scaler.transform(df_test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    876\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    879\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    822\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    859\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m         \u001b[0mfirst_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"n_samples_seen_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 861\u001b[0;31m         X = self._validate_data(\n\u001b[0m\u001b[1;32m    862\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m             \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    877\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    880\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m                 raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_array_api.py\u001b[0m in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"numpy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"numpy.array_api\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;31m# Use NumPy API to support order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    915\u001b[0m         \"\"\"\n\u001b[1;32m    916\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0musing_copy_on_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mastype_is_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(trainDocs['vectorized'])\n",
        "# X_test_scaled = scaler.transform(df_test)\n",
        "\n",
        "pca = PCA(n_components=0.95)  # Retain 95% of the variance\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "# X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "# 4. Examine the explained variance ratio\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "print(f'Explained variance ratio: {explained_variance_ratio}')\n",
        "print(f'Sum of explained variance ratio: {sum(explained_variance_ratio)}')\n",
        "\n",
        "# Optional: Convert PCA results back to DataFrame for easier handling\n",
        "df_train_pca = pd.DataFrame(X_train_pca, columns=[f'PC{i+1}' for i in range(X_train_pca.shape[1])])\n",
        "# df_test_pca = pd.DataFrame(X_test_pca, columns=[f'PC{i+1}' for i in range(X_test_pca.shape[1])])\n",
        "\n",
        "# Output the transformed data\n",
        "print(df_train_pca.head())\n",
        "# print(df_test_pca.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "HxtUnfMh_9IY",
        "outputId": "8c5866d0-2c33-4ca1-c589-7976e1ade7ad"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'pca' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-c57b6efdb14e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcomponents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# To find the top n features contributing to each component\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m  \u001b[0;31m# Example: top 1 features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mselected_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pca' is not defined"
          ]
        }
      ],
      "source": [
        "components = pca.components_\n",
        "\n",
        "# To find the top n features contributing to each component\n",
        "n = 10  # Example: top 1 features\n",
        "selected_features = []\n",
        "for i, component in enumerate(components):\n",
        "    # Get indices of the top n absolute loadings\n",
        "    indices = np.argsort(np.abs(component))[-n:]\n",
        "    selected_features.append(indices)\n",
        "    # print(f\"Top {n} features for principal component {i+1}: {indices}\")\n",
        "\n",
        "# Optional: Flatten the list if you want a single set of unique feature indices\n",
        "selected_features_flat = np.unique(np.concatenate(selected_features))\n",
        "selected_features_flat.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zIzc020gC736"
      },
      "outputs": [],
      "source": [
        "df_train_pca"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xq4dSl5xV_x"
      },
      "source": [
        "# **Token Tuples**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8k622ETXx6zv"
      },
      "source": [
        "**Train Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvqaNvaAxUhh",
        "outputId": "b3fe7480-27d9-403a-a125-b882b1dbe048"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuple_2 unique tokens: 47929\n"
          ]
        }
      ],
      "source": [
        "# pairs of terms (size 2)\n",
        "trainDocs['tuple_2'] = trainDocs['vectorized'].apply(lambda lst: make_tuple(lst, 2))\n",
        "# tuple of size 3\n",
        "# trainDocs['tuple_3'] = trainDocs['vectorized'].apply(lambda lst: make_tuple(lst, 3))\n",
        "\n",
        "# get list of unique tokens\n",
        "unique_token_pairs = get_unique_token_pairs(trainDocs['tuple_2'])\n",
        "print('Tuple_2 unique tokens: ' + str(len(unique_token_pairs)))\n",
        "\n",
        "# get list of unique tokens\n",
        "# unique_token_pairs_3 = get_unique_token_pairs(trainDocs['tuple_3'])\n",
        "# print('Tuple_3 unique tokens: ' + str(len(unique_token_pairs_3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d64-0fDx9PK"
      },
      "source": [
        "**Validation Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVIBl9Gcx_t6"
      },
      "outputs": [],
      "source": [
        "valDocs['tuple_2'] = valDocs['vectorized'].apply(lambda lst: make_tuple(lst, 2))\n",
        "# valDocs['tuple_3'] = valDocs['vectorized'].apply(lambda lst: make_tuple(lst, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JlshDBtWeS8"
      },
      "source": [
        "# **Count the number of Tuples**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2Tzp-ReWrWf"
      },
      "source": [
        "**Train Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SoYJ1pTvWXqs"
      },
      "outputs": [],
      "source": [
        "# Tuple Size 2\n",
        "pair_token_counts = []\n",
        "for pair in unique_token_pairs:\n",
        "    pair_token_counts.append(trainDocs['tuple_2'].apply(lambda lst: get_token_pairs_count(pair, lst)).to_numpy())\n",
        "\n",
        "# create a DataFrame of zeros with the tokens as column names\n",
        "pair_tokens_df = pd.DataFrame(pair_token_counts).T\n",
        "pair_tokens_df.index = trainDocs.index\n",
        "pair_tokens_df.columns = unique_token_pairs\n",
        "\n",
        "# Tuple Size 3\n",
        "# pair_3_token_counts = []\n",
        "# for pair in unique_token_pairs_3:\n",
        "#     pair_3_token_counts.append(trainDocs['tuple_3'].apply(lambda lst: get_token_pairs_count(pair, lst)).to_numpy())\n",
        "\n",
        "# create a DataFrame of zeros with the tokens as column names\n",
        "# pair_3_tokens_df = pd.DataFrame(pair_3_token_counts).T\n",
        "# pair_3_tokens_df.index = trainDocs.index\n",
        "# pair_3_tokens_df.columns = unique_token_pairs_3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5ZvfYq3WvBN"
      },
      "source": [
        "**Validation Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8mkGtF-Woj4"
      },
      "outputs": [],
      "source": [
        "pair_token_counts = []\n",
        "for pair in unique_token_pairs:\n",
        "    pair_token_counts.append(valDocs['tuple_2'].apply(lambda lst: get_token_pairs_count(pair, lst)).to_numpy())\n",
        "\n",
        "# create a DataFrame of zeros with the tokens as column names\n",
        "val_pair_tokens_df = pd.DataFrame(pair_token_counts).T\n",
        "val_pair_tokens_df.index = valDocs.index\n",
        "val_pair_tokens_df.columns = unique_token_pairs\n",
        "\n",
        "\n",
        "# Tuple Size 3\n",
        "# pair_3_token_counts = []\n",
        "# for pair in unique_token_pairs_3:\n",
        "#     pair_3_token_counts.append(valDocs['tuple_3'].apply(lambda lst: get_token_pairs_count(pair, lst)).to_numpy())\n",
        "\n",
        "# create a DataFrame of zeros with the tokens as column names\n",
        "# val_pair_3_tokens_df = pd.DataFrame(pair_3_token_counts).T\n",
        "# val_pair_3_tokens_df.index = valDocs.index\n",
        "# val_pair_3_tokens_df.columns = unique_token_pairs_3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mTgq1LFo4VI"
      },
      "source": [
        "# **Feature Selection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obI0jSFGqwze"
      },
      "source": [
        "**Train Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVqILc0Fo3-t",
        "outputId": "9ace088d-4431-4e3c-fe2f-366fba51a52b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of unique tuples-2 (features):4792\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-126-7ea83a993ba8>:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_selected_pairs_df['joined'] = train_selected_pairs_df.apply(create_list, axis=1)\n"
          ]
        }
      ],
      "source": [
        "# Tuple 2\n",
        "topics_array=trainTopics['topics_lst'].apply(lambda lst: lst[0]).values\n",
        "\n",
        "selected_pairs = feature_selection_rfe(pair_tokens_df, topics_array)\n",
        "print(\"No. of unique tuples-2 (features):\" + str(len(selected_pairs)))\n",
        "\n",
        "train_selected_pairs_df = pair_tokens_df[selected_pairs]\n",
        "train_selected_pairs_df['joined'] = train_selected_pairs_df.apply(create_list, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtPSxCUsgfkb"
      },
      "outputs": [],
      "source": [
        "# tuple 3\n",
        "# selected_pairs_3 = feature_selection_rfe(pair_3_tokens_df, topics_array)\n",
        "# print(\"No. of unique tuples-3 (features):\" + str(len(selected_pairs_3)))\n",
        "\n",
        "# train_selected_pairs_3_df = pair_3_tokens_df[selected_pairs_3]\n",
        "# train_selected_pairs_3_df['joined'] = train_selected_pairs_3_df.apply(create_list, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILV63fYGKgWs"
      },
      "source": [
        "**Validation Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUplRwjDq22G",
        "outputId": "37afda6b-c276-4139-8a25-fb43f0928c92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-128-bffb282f83cb>:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  val_selected_pairs_df['joined'] = val_selected_pairs_df.apply(create_list, axis=1)\n"
          ]
        }
      ],
      "source": [
        "# Tuple 2\n",
        "val_selected_pairs_df = pd.DataFrame(0, index=valDocs.index, columns=selected_pairs, dtype=int)\n",
        "for col in selected_pairs:\n",
        "    if col in val_pair_tokens_df.columns:\n",
        "        val_selected_pairs_df[col] = val_pair_tokens_df[col]\n",
        "\n",
        "val_selected_pairs_df['joined'] = val_selected_pairs_df.apply(create_list, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90sriiZXrfS3"
      },
      "outputs": [],
      "source": [
        "# Tuple 3\n",
        "# val_selected_pairs_3_df = pd.DataFrame(0, index=valDocs.index, columns=selected_pairs_3, dtype=int)\n",
        "# for col in selected_pairs_3:\n",
        "#     if col in val_pair_3_tokens_df.columns:\n",
        "#         val_selected_pairs_3_df[col] = val_pair_3_tokens_df[col]\n",
        "\n",
        "# val_selected_pairs_3_df['joined'] = val_selected_pairs_3_df.apply(create_list, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3McFDJP3-RLL"
      },
      "source": [
        "# **POS Tagging**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ci3hMOTnmUmr"
      },
      "source": [
        "**Train**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61sguY83-Q-C"
      },
      "outputs": [],
      "source": [
        "trainDocs['pos_tag'] = trainDocs['preprocess'].apply(lambda lst: lemmatize(lst))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIFumiS_mXRQ"
      },
      "source": [
        "**Validation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJd4owdumXzM"
      },
      "outputs": [],
      "source": [
        "valDocs['pos_tag'] = valDocs['preprocess'].apply(lambda lst: lemmatize(lst))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TS8FntbK_tEE"
      },
      "source": [
        "# **Padding**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKGX3poKmlmy"
      },
      "source": [
        "**Train Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oklFR2ns_rN_"
      },
      "outputs": [],
      "source": [
        "trainDocs['token_padded'] = trainDocs['vectorized'].apply(lambda lst: padding(lst, maximum_length))\n",
        "trainDocs['pos_padded'] = trainDocs['pos_tag'].apply(lambda lst: padding(lst, maximum_length))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsRlqysymoMl"
      },
      "source": [
        "**Validation Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3T5JXLRmoA7"
      },
      "outputs": [],
      "source": [
        "valDocs['token_padded'] = valDocs['vectorized'].apply(lambda lst: padding(lst, maximum_length))\n",
        "valDocs['pos_padded'] = valDocs['pos_tag'].apply(lambda lst: padding(lst, maximum_length))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKzCk13OU5f4"
      },
      "source": [
        "# **GloVe Embedding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIFcY-TXU4Ik"
      },
      "outputs": [],
      "source": [
        "embedding_matrix = GloVe_embedding(count_vectorizer.vocabulary_, vocab_size, embedding_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2uBDhkMwtti"
      },
      "source": [
        "# **Test Data Prepration**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGfQIOoAwqwp"
      },
      "source": [
        "**Vectorize Test Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8hbj2QTOisn"
      },
      "outputs": [],
      "source": [
        "testDocs['vectorized'] = testDocs['preprocess'].apply(lambda lst: vectorize(tfidf_vectorizer.vocabulary_, lst))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMrVhACYoRQk"
      },
      "source": [
        "**Test TF and TF-iDF**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BQNGP59PW3D"
      },
      "outputs": [],
      "source": [
        "test_tf_matrix = count_vectorizer.transform(testDocs['joined_tokens'])\n",
        "test_tf = pd.DataFrame(test_tf_matrix[:, unique_terms].toarray(), columns=unique_terms, index=testDocs.index)\n",
        "testDocs['tf'] = pd.DataFrame(test_tf.apply(create_list, axis=1)).iloc[:, 0].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7e8yBaUCbhIA"
      },
      "outputs": [],
      "source": [
        "test_tfidf_matrix = tfidf_vectorizer.transform(testDocs['joined_tokens'])\n",
        "test_tfidf = pd.DataFrame(test_tfidf_matrix[:, unique_terms].toarray(), columns=unique_terms, index=testDocs.index)\n",
        "testDocs['tfidf'] = pd.DataFrame(test_tfidf.apply(create_list, axis=1)).iloc[:, 0].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9hFRRawoZLD"
      },
      "source": [
        "**Test Term Dictionary**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPRK4u5MxkVl"
      },
      "outputs": [],
      "source": [
        "testTTW = pd.DataFrame(None, columns=range(len(favorite_topics)), index=testDocs.index)\n",
        "\n",
        "for topic in list(token_topic_dict.columns):\n",
        "    testTTW[topic] = testDocs['vectorized'].apply(lambda lst: [token_topic_dict.loc[term][topic] if term in token_topic_dict.index else 0 for term in lst])\n",
        "\n",
        "for col in list(testTTW.columns):\n",
        "    testTTW[col] = testTTW[col].apply(lambda lst: padding(lst, maximum_length))\n",
        "\n",
        "# Apply the function and create a new column\n",
        "columns = testTTW.columns\n",
        "testTTW['concatinated'] = testTTW.apply(lambda lst: concatenate_arrays(columns, lst),  axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpLtbFXQoghD"
      },
      "source": [
        "**Test Term Pairs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7vcssN6oigk"
      },
      "outputs": [],
      "source": [
        "testDocs['tuple_2'] = testDocs['vectorized'].apply(lambda lst: make_tuple(lst, 2))\n",
        "\n",
        "pair_token_counts = []\n",
        "for pair in unique_token_pairs:\n",
        "    pair_token_counts.append(testDocs['tuple_2'].apply(lambda lst: get_token_pairs_count(pair, lst)).to_numpy())\n",
        "\n",
        "# create a DataFrame of zeros with the tokens as column names\n",
        "test_pair_tokens_df = pd.DataFrame(pair_token_counts).T\n",
        "test_pair_tokens_df.index = testDocs.index\n",
        "test_pair_tokens_df.columns = unique_token_pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXJtYGWDzxCq"
      },
      "outputs": [],
      "source": [
        "# testDocs['tuple_3'] = testDocs['vectorized'].apply(lambda lst: make_tuple(lst, 3))\n",
        "\n",
        "# pair_3_token_counts = []\n",
        "# for pair in unique_token_pairs:\n",
        "#     pair_3_token_counts.append(testDocs['tuple_3'].apply(lambda lst: get_token_pairs_count(pair, lst)).to_numpy())\n",
        "\n",
        "# # create a DataFrame of zeros with the tokens as column names\n",
        "# test_pair_3_tokens_df = pd.DataFrame(pair_3_token_counts).T\n",
        "# test_pair_3_tokens_df.index = testDocs.index\n",
        "# test_pair_3_tokens_df.columns = unique_token_pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NX8KuScWJGz",
        "outputId": "f4909374-c9b1-4259-9102-09ddf6ca3e5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-141-54723e366200>:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  test_selected_pairs_df['joined'] = test_selected_pairs_df.apply(create_list, axis=1)\n"
          ]
        }
      ],
      "source": [
        "test_selected_pairs_df = pd.DataFrame(0, index=testDocs.index, columns=selected_pairs, dtype=int)\n",
        "for col in selected_pairs:\n",
        "    if col in test_pair_tokens_df.columns:\n",
        "        test_selected_pairs_df[col] = test_pair_tokens_df[col]\n",
        "\n",
        "test_selected_pairs_df['joined'] = test_selected_pairs_df.apply(create_list, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unpaUdj90ygz"
      },
      "outputs": [],
      "source": [
        "# test_selected_pairs_3_df = pd.DataFrame(0, index=testDocs.index, columns=selected_pairs_3, dtype=int)\n",
        "# for col in selected_pairs_3:\n",
        "#     if col in test_pair_3_tokens_df.columns:\n",
        "#         test_selected_pairs_3_df[col] = test_pair_3_tokens_df[col]\n",
        "\n",
        "# test_selected_pairs_3_df['joined'] = test_selected_pairs_3_df.apply(create_list, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcdbdQo5xH38"
      },
      "source": [
        "**POS Tagging Test Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRN5PXx-xIae"
      },
      "outputs": [],
      "source": [
        "testDocs['pos_tag'] = testDocs['preprocess'].apply(lambda lst: lemmatize(lst))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0117O04xmrw"
      },
      "source": [
        "**Padding Test Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgI33XicxkiO"
      },
      "outputs": [],
      "source": [
        "testDocs['token_padded'] = testDocs['vectorized'].apply(lambda lst: padding(lst, maximum_length))\n",
        "testDocs['pos_padded'] = testDocs['pos_tag'].apply(lambda lst: padding(lst, maximum_length))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIZwc1mTyJvv"
      },
      "source": [
        "**Dataframe to Array**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4SsXrx3m8GC"
      },
      "source": [
        "# **Convert Features To Arrays**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0l4WCCLm1o7"
      },
      "source": [
        "**Train Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTp8s5caeNXS"
      },
      "outputs": [],
      "source": [
        "trainDocs_X = np.array(trainDocs['token_padded'].tolist())\n",
        "trainPos_X = np.array(trainDocs['pos_padded'].tolist())\n",
        "trainTf_X = np.array(trainDocs['tf'].tolist())\n",
        "trainTfidf_X = np.array(trainDocs['tfidf'].tolist())\n",
        "trainTermDict_X = np.array(trainTTW['concatinated'].tolist())\n",
        "trainPairs_X =  np.array(train_selected_pairs_df['joined'].tolist())\n",
        "# trainPairs_3_X =  np.array(train_selected_pairs_3_df['joined'].tolist())\n",
        "train_Y = np.array(trainTopics['one_hot'].tolist())\n",
        "\n",
        "# convert pairs into np.array\n",
        "# trainDocs_tuple_2 = np.array([np.array(pair) for doc in trainDocs['tuple_2'] for pair in doc])\n",
        "# trainDocs_tuple_3 = np.array([np.array(pair) for doc in trainDocs['tuple_3'] for pair in doc])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRgaO-doivBR",
        "outputId": "e6c96631-ccc6-4663-f30c-bf026f4e2e7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1279, 100)\n",
            "(1279, 100)\n",
            "(1279, 4792)\n"
          ]
        }
      ],
      "source": [
        "print(trainDocs_X.shape)\n",
        "print(trainPos_X.shape)\n",
        "print(trainPairs_X.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzFKIKKom304"
      },
      "source": [
        "**Validation Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Dr4yBlCm3ld"
      },
      "outputs": [],
      "source": [
        "valDocs_X = np.array(valDocs['token_padded'].tolist())\n",
        "valPos_X = np.array(valDocs['pos_padded'].tolist())\n",
        "valTf_X = np.array(valDocs['tf'].tolist())\n",
        "valTfidf_X = np.array(valDocs['tfidf'].tolist())\n",
        "valTermDict_X = np.array(valTTW['concatinated'].tolist())\n",
        "valPairs_X =  np.array(val_selected_pairs_df['joined'].tolist())\n",
        "# valPairs_3_X =  np.array(val_selected_pairs_3_df['joined'].tolist())\n",
        "val_Y = np.array(valTopcis['one_hot'].tolist())\n",
        "\n",
        "# convert pairs into np.array\n",
        "# valDocs_tuple_2 = np.array([np.array(pair) for doc in valDocs['tuple_2'] for pair in doc])\n",
        "# valDocs_tuple_3 = np.array([np.array(pair) for doc in valDocs['tuple_3'] for pair in doc])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sKhsOa4pKob"
      },
      "source": [
        "**Test Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6zlRIfPyJiK"
      },
      "outputs": [],
      "source": [
        "testDocs_X = np.array(testDocs['token_padded'].tolist())\n",
        "testPos_X = np.array(testDocs['pos_padded'].tolist())\n",
        "testTf_X = np.array(testDocs['tf'].tolist())\n",
        "testTfidf_X = np.array(testDocs['tfidf'].tolist())\n",
        "testTermDict_X = np.array(testTTW['concatinated'].tolist())\n",
        "testPairs_X =  np.array(test_selected_pairs_df['joined'].tolist())\n",
        "# testPairs_3_X =  np.array(test_selected_pairs_3_df['joined'].tolist())\n",
        "test_y = np.array(testTopics['one_hot'].tolist())\n",
        "label_y = np.argmax(test_y, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJRimjYJnGqf"
      },
      "source": [
        "# **Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2MdrvaBppHY"
      },
      "source": [
        "## CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsBLMcyXpofN"
      },
      "outputs": [],
      "source": [
        "def cnn_model():\n",
        "    text_input = Input(shape=(maximum_length,))\n",
        "    embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(text_input)\n",
        "\n",
        "    # embedding_layer = Embedding(input_dim=vocab_size,\n",
        "    #                             output_dim=embedding_dim,\n",
        "    #                             weights=[embedding_matrix],\n",
        "    #                             input_length=maximum_length,\n",
        "    #                             trainable=False)(text_input)\n",
        "\n",
        "    # Convolutional layers for text processing\n",
        "    conv_layer_1 = Conv1D(filters=256, kernel_size=5, activation='relu')(embedding_layer)\n",
        "    pooling_layer_1 = MaxPooling1D(pool_size=2)(conv_layer_1)\n",
        "    conv_layer_2 = Conv1D(filters=128, kernel_size=5, activation='relu')(pooling_layer_1)\n",
        "    pooling_layer_2 = MaxPooling1D(pool_size=2)(conv_layer_2)\n",
        "    conv_layer_3 = Conv1D(filters=64, kernel_size=5, activation='relu')(pooling_layer_2)\n",
        "    pooling_layer_3 = MaxPooling1D(pool_size=2)(conv_layer_3)\n",
        "    flatten_layer = Flatten()(pooling_layer_3)\n",
        "\n",
        "    # POS tags input for extra features\n",
        "    pos_layer_input = Input(shape=(num_extra_features,1))\n",
        "    pos_conv_layer_1 = Conv1D(filters=64, kernel_size=5, activation='relu')(pos_layer_input)\n",
        "    pos_pooling_layer_1 = MaxPooling1D(pool_size=2)(pos_conv_layer_1)\n",
        "    pos_conv_layer_2 = Conv1D(filters=32, kernel_size=5, activation='relu')(pos_pooling_layer_1)\n",
        "    pos_pooling_layer_2 = MaxPooling1D(pool_size=2)(pos_conv_layer_2)\n",
        "    pos_flatten_layer = Flatten()(pos_pooling_layer_2)\n",
        "    # dense_pos_layer = Dense(512, activation='relu')(pos_layer_input)\n",
        "\n",
        "    num_unique_terms = len(unique_terms)\n",
        "    tf_layer_input = Input(shape=(num_unique_terms,1))\n",
        "    tf_conv_layer_1 = Conv1D(filters=128, kernel_size=5, activation='relu')(tf_layer_input)\n",
        "    tf_pooling_layer_1 = MaxPooling1D(pool_size=2)(tf_conv_layer_1)\n",
        "    tf_conv_layer_2 = Conv1D(filters=64, kernel_size=5, activation='relu')(tf_pooling_layer_1)\n",
        "    tf_pooling_layer_2 = MaxPooling1D(pool_size=2)(tf_conv_layer_2)\n",
        "    tf_conv_layer_3 = Conv1D(filters=32, kernel_size=5, activation='relu')(tf_pooling_layer_2)\n",
        "    tf_pooling_layer_3 = MaxPooling1D(pool_size=2)(tf_conv_layer_3)\n",
        "    tf_flatten_layer = Flatten()(tf_pooling_layer_3)\n",
        "    # dense_tf_layer = Dense(512, activation='relu')(tf_layer_input)\n",
        "\n",
        "    # num_unique_terms = len(unique_terms)\n",
        "    # tfidf_layer_input = Input(shape=(num_unique_terms,1))\n",
        "    # tfidf_conv_layer_1 = Conv1D(filters=128, kernel_size=5, activation='relu')(tfidf_layer_input)\n",
        "    # tfidf_pooling_layer_1 = MaxPooling1D(pool_size=2)(tfidf_conv_layer_1)\n",
        "    # tfidf_conv_layer_2 = Conv1D(filters=64, kernel_size=5, activation='relu')(tfidf_pooling_layer_1)\n",
        "    # tfidf_pooling_layer_2 = MaxPooling1D(pool_size=2)(tfidf_conv_layer_2)\n",
        "    # tfidf_conv_layer_3 = Conv1D(filters=32, kernel_size=5, activation='relu')(tfidf_pooling_layer_2)\n",
        "    # tfidf_pooling_layer_3 = MaxPooling1D(pool_size=2)(tfidf_conv_layer_3)\n",
        "    # tfidf_flatten_layer = Flatten()(tfidf_pooling_layer_3)\n",
        "    # dense_tfidf_layer = Dense(512, activation='relu')(tfidf_layer_input)\n",
        "\n",
        "    # Dictionary of terms layer\n",
        "    # doc_term_dic_length = 4 * maximum_length\n",
        "    # term_dict_layer_input = Input(shape=(doc_term_dic_length,1))\n",
        "    # term_conv_layer_1 = Conv1D(filters=256, kernel_size=5, activation='relu')(term_dict_layer_input)\n",
        "    # term_pooling_layer_1 = MaxPooling1D(pool_size=3)(term_conv_layer_1)\n",
        "    # term_conv_layer_2 = Conv1D(filters=128, kernel_size=5, activation='relu')(term_pooling_layer_1)\n",
        "    # term_pooling_layer_2 = MaxPooling1D(pool_size=2)(term_conv_layer_2)\n",
        "    # term_conv_layer_3 = Conv1D(filters=64, kernel_size=5, activation='relu')(term_pooling_layer_2)\n",
        "    # term_pooling_layer_3 = MaxPooling1D(pool_size=2)(term_conv_layer_3)\n",
        "    # term_flatten_layer = Flatten()(term_pooling_layer_3)\n",
        "    # dense_term_dict_layer = Dense(512, activation='relu')(term_dict_layer_input)\n",
        "\n",
        "    # pairs input for extra features\n",
        "    pair_layer_input = Input(shape=(len(selected_pairs),1))\n",
        "    pair_conv_layer_1 = Conv1D(filters=256, kernel_size=3, activation='relu')(pair_layer_input)\n",
        "    pair_pooling_layer_1 = MaxPooling1D(pool_size=3)(pair_conv_layer_1)\n",
        "    pair_conv_layer_2 = Conv1D(filters=128, kernel_size=3, activation='relu')(pair_pooling_layer_1)\n",
        "    pair_pooling_layer_2 = MaxPooling1D(pool_size=3)(pair_conv_layer_2)\n",
        "    pair_conv_layer_3 = Conv1D(filters=64, kernel_size=3, activation='relu')(pair_pooling_layer_2)\n",
        "    pair_pooling_layer_3 = MaxPooling1D(pool_size=3)(pair_conv_layer_3)\n",
        "    pair_flatten_layer = Flatten()(pair_pooling_layer_3)\n",
        "    # dense_pair_layer = Dense(512, activation='relu')(pair_flatten_layer)\n",
        "\n",
        "    merged = concatenate([flatten_layer, pos_flatten_layer, tf_flatten_layer, pair_flatten_layer])\n",
        "\n",
        "    # Additional layers for further processing\n",
        "    dense_layer_1 = Dense(128, activation='relu')(merged)\n",
        "    dropout_layer_1 = Dropout(0.1)(dense_layer_1)\n",
        "    dense_layer_2 = Dense(64, activation='relu')(dropout_layer_1)\n",
        "    dropout_layer_2 = Dropout(0.1)(dense_layer_2)\n",
        "    dense_layer_3 = Dense(32, activation='relu')(dropout_layer_2)\n",
        "    output_layer = Dense(num_classes, activation='softmax')(dense_layer_3)\n",
        "\n",
        "    # Define the model\n",
        "    model = Model(inputs=[text_input, pos_layer_input, tf_layer_input, pair_layer_input], outputs=output_layer)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'],)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDlJrCw-qVGT"
      },
      "source": [
        "## CNN With Repetition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4V5g8xD-Sao"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTMhzdmoof_3",
        "outputId": "385a3272-a97f-4dc3-fe4a-b76a40bea8c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1\n",
            "Epoch 1/5\n",
            "20/20 [==============================] - 207s 10s/step - loss: 0.6433 - accuracy: 0.7756 - val_loss: 0.2992 - val_accuracy: 0.9125\n",
            "Epoch 2/5\n",
            "20/20 [==============================] - 187s 9s/step - loss: 0.2316 - accuracy: 0.9531 - val_loss: 0.3615 - val_accuracy: 0.9375\n",
            "Epoch 3/5\n",
            "20/20 [==============================] - 189s 9s/step - loss: 0.2616 - accuracy: 0.9742 - val_loss: 0.3765 - val_accuracy: 0.9406\n",
            "Epoch 4/5\n",
            "20/20 [==============================] - 177s 9s/step - loss: 0.7346 - accuracy: 0.9531 - val_loss: 1.2078 - val_accuracy: 0.9469\n",
            "Epoch 5/5\n",
            "20/20 [==============================] - 189s 10s/step - loss: 1.8641 - accuracy: 0.9601 - val_loss: 6.1893 - val_accuracy: 0.9312\n",
            "10/10 [==============================] - 8s 726ms/step\n",
            "13/13 [==============================] - 17s 1s/step\n",
            "Iteration 2\n",
            "Epoch 1/5\n",
            "20/20 [==============================] - 193s 10s/step - loss: 0.6272 - accuracy: 0.7694 - val_loss: 0.2234 - val_accuracy: 0.9344\n",
            "Epoch 2/5\n",
            "20/20 [==============================] - 176s 9s/step - loss: 0.1659 - accuracy: 0.9617 - val_loss: 0.3613 - val_accuracy: 0.9406\n",
            "Epoch 3/5\n",
            "20/20 [==============================] - 199s 10s/step - loss: 0.2925 - accuracy: 0.9758 - val_loss: 0.7528 - val_accuracy: 0.9438\n",
            "Epoch 4/5\n",
            "20/20 [==============================] - 190s 10s/step - loss: 1.6066 - accuracy: 0.9586 - val_loss: 1.5287 - val_accuracy: 0.9594\n",
            "Epoch 5/5\n",
            "20/20 [==============================] - 187s 9s/step - loss: 3.4780 - accuracy: 0.9633 - val_loss: 15.4966 - val_accuracy: 0.9031\n",
            "10/10 [==============================] - 15s 1s/step\n",
            "13/13 [==============================] - 19s 1s/step\n",
            "Iteration 3\n",
            "Epoch 1/5\n",
            "20/20 [==============================] - 201s 10s/step - loss: 0.6427 - accuracy: 0.7772 - val_loss: 0.3172 - val_accuracy: 0.9375\n",
            "Epoch 2/5\n",
            "20/20 [==============================] - 179s 9s/step - loss: 0.1840 - accuracy: 0.9664 - val_loss: 0.2311 - val_accuracy: 0.9469\n",
            "Epoch 3/5\n",
            "20/20 [==============================] - 189s 10s/step - loss: 0.0920 - accuracy: 0.9867 - val_loss: 0.4868 - val_accuracy: 0.9469\n",
            "Epoch 4/5\n",
            "20/20 [==============================] - 198s 10s/step - loss: 0.3493 - accuracy: 0.9773 - val_loss: 2.5956 - val_accuracy: 0.9344\n",
            "Epoch 5/5\n",
            "20/20 [==============================] - 201s 10s/step - loss: 1.8806 - accuracy: 0.9789 - val_loss: 7.5041 - val_accuracy: 0.9438\n",
            "10/10 [==============================] - 10s 1s/step\n",
            "13/13 [==============================] - 17s 1s/step\n",
            "Iteration 4\n",
            "Epoch 1/5\n",
            "20/20 [==============================] - 199s 10s/step - loss: 0.6442 - accuracy: 0.7819 - val_loss: 0.3108 - val_accuracy: 0.9187\n",
            "Epoch 2/5\n",
            "20/20 [==============================] - 208s 10s/step - loss: 0.2204 - accuracy: 0.9609 - val_loss: 0.2478 - val_accuracy: 0.9563\n",
            "Epoch 3/5\n",
            "20/20 [==============================] - 194s 10s/step - loss: 0.3766 - accuracy: 0.9672 - val_loss: 1.0300 - val_accuracy: 0.9438\n",
            "Epoch 4/5\n",
            "20/20 [==============================] - 194s 10s/step - loss: 2.7623 - accuracy: 0.9476 - val_loss: 6.4541 - val_accuracy: 0.9187\n",
            "Epoch 5/5\n",
            "20/20 [==============================] - 194s 10s/step - loss: 5.8264 - accuracy: 0.9507 - val_loss: 14.6379 - val_accuracy: 0.9531\n",
            "10/10 [==============================] - 21s 1s/step\n",
            "13/13 [==============================] - 18s 1s/step\n",
            "Iteration 5\n",
            "Epoch 1/5\n",
            "20/20 [==============================] - 193s 10s/step - loss: 0.6610 - accuracy: 0.7795 - val_loss: 0.2330 - val_accuracy: 0.9438\n",
            "Epoch 2/5\n",
            "20/20 [==============================] - 191s 10s/step - loss: 0.2006 - accuracy: 0.9515 - val_loss: 0.2310 - val_accuracy: 0.9500\n",
            "Epoch 3/5\n",
            "20/20 [==============================] - 206s 10s/step - loss: 0.1989 - accuracy: 0.9789 - val_loss: 0.6079 - val_accuracy: 0.9594\n",
            "Epoch 4/5\n",
            "20/20 [==============================] - 198s 10s/step - loss: 1.0623 - accuracy: 0.9586 - val_loss: 5.5533 - val_accuracy: 0.8906\n",
            "Epoch 5/5\n",
            "20/20 [==============================] - 189s 10s/step - loss: 4.1669 - accuracy: 0.9523 - val_loss: 16.6369 - val_accuracy: 0.9219\n",
            "10/10 [==============================] - 14s 1s/step\n",
            "13/13 [==============================] - 19s 1s/step\n",
            "Iteration 6\n",
            "Epoch 1/5\n",
            "20/20 [==============================] - 199s 10s/step - loss: 0.6525 - accuracy: 0.7647 - val_loss: 0.2651 - val_accuracy: 0.9312\n",
            "Epoch 2/5\n",
            "20/20 [==============================] - 193s 10s/step - loss: 0.1940 - accuracy: 0.9593 - val_loss: 0.3272 - val_accuracy: 0.9344\n",
            "Epoch 3/5\n",
            "20/20 [==============================] - 190s 10s/step - loss: 0.2752 - accuracy: 0.9703 - val_loss: 0.7125 - val_accuracy: 0.9438\n",
            "Epoch 4/5\n",
            "20/20 [==============================] - 211s 11s/step - loss: 1.2543 - accuracy: 0.9570 - val_loss: 3.1236 - val_accuracy: 0.9312\n",
            "Epoch 5/5\n",
            "15/20 [=====================>........] - ETA: 45s - loss: 3.2623 - accuracy: 0.9573"
          ]
        }
      ],
      "source": [
        "val_accuracies = []\n",
        "val_precisions = []\n",
        "val_recalls = []\n",
        "val_f1_scores = []\n",
        "accuracies = []\n",
        "precisions = []\n",
        "recalls = []\n",
        "f1_scores = []\n",
        "f1_scores_micro = []\n",
        "f1_scores_macro = []\n",
        "\n",
        "for i in range(10):\n",
        "    print(f'Iteration {i+1}')\n",
        "    model = cnn_model()\n",
        "    model.fit([trainDocs_X, trainPos_X, trainTf_X, trainPairs_X], train_Y, epochs=5, batch_size=64, validation_data=([valDocs_X, valPos_X, valTf_X, valPairs_X], val_Y))\n",
        "    val_predictions = model.predict([valDocs_X, valPos_X, valTf_X, valPairs_X])\n",
        "    val_true_labels = np.argmax(val_Y, axis=1)\n",
        "    val_pred_labels = np.argmax(val_predictions, axis=1)\n",
        "    val_precisions.append(precision_score(val_true_labels, val_pred_labels, average=None))\n",
        "    val_recalls.append(recall_score(val_true_labels, val_pred_labels, average=None))\n",
        "    val_f1_scores.append(f1_score(val_true_labels, val_pred_labels, average=None))\n",
        "    val_accuracies.append(accuracy_score(val_true_labels, val_pred_labels))\n",
        "\n",
        "    predictions = pd.DataFrame(model.predict([testDocs_X, testPos_X, testTf_X, testPairs_X]))\n",
        "    predictions['classes_probs'] = predictions.apply(lambda row: list(row), axis=1)\n",
        "    predictions['pred_label'] = predictions['classes_probs'].apply(lambda lst: (np.array(lst) >= 0.05).astype(int))\n",
        "\n",
        "    predicted_y = np.array(predictions['pred_label'].tolist())\n",
        "\n",
        "    accuracies.append(accuracy_score(test_y, predicted_y))\n",
        "    precisions.append(precision_score(test_y, predicted_y, average=None))\n",
        "    recalls.append(recall_score(test_y, predicted_y, average=None))\n",
        "    f1_scores.append(f1_score(test_y, predicted_y, average=None))\n",
        "    f1_scores_micro.append(f1_score(test_y, predicted_y, average='micro'))\n",
        "    f1_scores_macro.append(f1_score(test_y, predicted_y, average='macro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6eIV5eRvaxA",
        "outputId": "1d1fc22a-6893-44bd-c4a3-3d05d4ae776e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.913125\n",
            "f1_score: [0.88520736 0.75663534 0.87119052 0.94142929]\n",
            "precision: [0.88758673 0.99       0.88077728 0.9342286 ]\n",
            "recall: [0.886      0.66363636 0.87241379 0.95      ]\n",
            "\n",
            "\n",
            "test accuracy: 0.89375\n",
            "\n",
            "\n",
            "test f1_score: [0.92027779 0.81113527 0.82220716 0.94592888]\n",
            "test precision: [0.91642717 0.99166667 0.80196482 0.93298032]\n",
            "test recall: [0.928125   0.73076923 0.86052632 0.96086957]\n",
            "\n",
            "\n",
            "f1_score_micro: 0.922321564524894\n",
            "f1_score_macro: 0.8748872760218858\n"
          ]
        }
      ],
      "source": [
        "print(\"accuracy:\", np.mean(val_accuracies))\n",
        "print(\"f1_score:\", np.mean(val_f1_scores, axis=0))\n",
        "print(\"precision:\", np.mean(val_precisions, axis=0))\n",
        "print(\"recall:\", np.mean(val_recalls, axis=0))\n",
        "print(\"\\n\")\n",
        "print(\"test accuracy:\", np.mean(accuracies))\n",
        "print(\"\\n\")\n",
        "print(\"test f1_score:\", np.mean(f1_scores, axis=0))\n",
        "print(\"test precision:\", np.mean(precisions, axis=0))\n",
        "print(\"test recall:\", np.mean(recalls, axis=0))\n",
        "print(\"\\n\")\n",
        "print(\"f1_score_micro:\", np.mean(f1_scores_micro))\n",
        "print(\"f1_score_macro:\", np.mean(f1_scores_macro))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-L1Un9-ztGzJ"
      },
      "outputs": [],
      "source": [
        "model.save('cnn_exp21.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "z89m8-vn0gj9",
        "outputId": "ba5a0229-3796-49e0-dc2f-408751e37bd4"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_9192028e-44ce-4354-af9d-9d9da81e8a9d\", \"cnn_exp17.keras\", 98734861)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "files.download('cnn_exp17.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "We99-8wzw8DV"
      },
      "source": [
        "## CNN Single Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eI0lEmvtQVoU",
        "outputId": "0dea0575-f1b9-4c02-e3ab-6499703e28ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "20/20 [==============================] - 26s 725ms/step - loss: 0.7970 - accuracy: 0.6896 - val_loss: 0.5629 - val_accuracy: 0.7688\n",
            "Epoch 2/5\n",
            "20/20 [==============================] - 19s 954ms/step - loss: 0.2902 - accuracy: 0.9023 - val_loss: 0.4036 - val_accuracy: 0.8875\n",
            "Epoch 3/5\n",
            "20/20 [==============================] - 17s 910ms/step - loss: 0.1258 - accuracy: 0.9672 - val_loss: 0.3770 - val_accuracy: 0.8969\n",
            "Epoch 4/5\n",
            "20/20 [==============================] - 12s 586ms/step - loss: 0.0752 - accuracy: 0.9891 - val_loss: 0.4387 - val_accuracy: 0.9125\n",
            "Epoch 5/5\n",
            "20/20 [==============================] - 19s 990ms/step - loss: 0.1015 - accuracy: 0.9906 - val_loss: 0.6339 - val_accuracy: 0.8906\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x792e4ef2f730>"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_input = Input(shape=(maximum_length,))\n",
        "embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(text_input)\n",
        "\n",
        "# embedding_layer = Embedding(input_dim=vocab_size,\n",
        "#                             output_dim=embedding_dim,\n",
        "#                             weights=[embedding_matrix],\n",
        "#                             input_length=maximum_length,\n",
        "#                             trainable=False)(text_input)\n",
        "\n",
        "# Convolutional layers for text processing\n",
        "conv_layer_1 = Conv1D(filters=256, kernel_size=5, activation='relu')(embedding_layer)\n",
        "pooling_layer_1 = MaxPooling1D(pool_size=2)(conv_layer_1)\n",
        "conv_layer_2 = Conv1D(filters=128, kernel_size=5, activation='relu')(pooling_layer_1)\n",
        "pooling_layer_2 = MaxPooling1D(pool_size=2)(conv_layer_2)\n",
        "conv_layer_3 = Conv1D(filters=64, kernel_size=5, activation='relu')(pooling_layer_2)\n",
        "pooling_layer_3 = MaxPooling1D(pool_size=2)(conv_layer_3)\n",
        "flatten_layer = Flatten()(pooling_layer_3)\n",
        "\n",
        "# POS tags input for extra features\n",
        "pos_layer_input = Input(shape=(num_extra_features,))\n",
        "dense_pos_layer = Dense(512, activation='relu')(pos_layer_input)\n",
        "\n",
        "# pairs input for extra features\n",
        "pair_layer_input = Input(shape=(len(selected_pairs),))\n",
        "dense_pair_layer = Dense(512, activation='relu')(pair_layer_input)\n",
        "\n",
        "merged = concatenate([flatten_layer, dense_pos_layer, dense_pair_layer])\n",
        "\n",
        "# Additional layers for further processing\n",
        "dense_layer_1 = Dense(128, activation='relu')(merged)\n",
        "dropout_layer_1 = Dropout(0.1)(dense_layer_1)\n",
        "dense_layer_2 = Dense(64, activation='relu')(dropout_layer_1)\n",
        "dropout_layer_2 = Dropout(0.1)(dense_layer_2)\n",
        "dense_layer_3 = Dense(32, activation='relu')(dropout_layer_2)\n",
        "output_layer = Dense(num_classes, activation='softmax')(dense_layer_3)\n",
        "\n",
        "# Define the model\n",
        "model = Model(inputs=[text_input, pos_layer_input, pair_layer_input], outputs=output_layer)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'],)\n",
        "\n",
        "# Train the model\n",
        "model.fit([trainDocs_X, trainPos_X, trainPairs_X], train_Y, epochs=5, batch_size=64, validation_data=([valDocs_X, valPos_X, valPairs_X], val_Y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoeONbnNw-ww"
      },
      "source": [
        "## MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXzdfEipIDk3",
        "outputId": "b70d22e2-b1cc-44b7-c123-6fd590eafafe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "20/20 [==============================] - 80s 4s/step - loss: 1.0861 - accuracy: 0.5496 - val_loss: 0.7524 - val_accuracy: 0.7125\n",
            "Epoch 2/5\n",
            "20/20 [==============================] - 69s 3s/step - loss: 0.5295 - accuracy: 0.8069 - val_loss: 0.4386 - val_accuracy: 0.8469\n",
            "Epoch 3/5\n",
            "20/20 [==============================] - 66s 3s/step - loss: 0.2633 - accuracy: 0.9187 - val_loss: 0.4066 - val_accuracy: 0.8687\n",
            "Epoch 4/5\n",
            "20/20 [==============================] - 71s 4s/step - loss: 0.1515 - accuracy: 0.9507 - val_loss: 0.4634 - val_accuracy: 0.8844\n",
            "Epoch 5/5\n",
            "20/20 [==============================] - 69s 4s/step - loss: 0.1253 - accuracy: 0.9679 - val_loss: 0.6555 - val_accuracy: 0.8938\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x792e44096140>"
            ]
          },
          "execution_count": 98,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# main text input\n",
        "text_input = Input(shape=(maximum_length,))\n",
        "embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(text_input)\n",
        "lstm_layer = LSTM(units=512)(embedding_layer)\n",
        "\n",
        "# text_input = Input(shape=(maximum_length,))\n",
        "# embedding_layer = Embedding(input_dim=vocab_size,\n",
        "#                             output_dim=embedding_dim,\n",
        "#                             weights=[embedding_matrix],\n",
        "#                             input_length=maximum_length,\n",
        "#                             trainable=False)(text_input)\n",
        "# lstm_layer = LSTM(units=1024)(embedding_layer)\n",
        "\n",
        "\n",
        "# POS tags input for extra features\n",
        "pos_layer_input = Input(shape=(num_extra_features,))\n",
        "dense_pos_layer = Dense(512, activation='relu')(pos_layer_input)\n",
        "\n",
        "# TF input for extra features\n",
        "num_unique_terms = len(unique_terms)\n",
        "tf_layer_input = Input(shape=(num_unique_terms,))\n",
        "dense_tf_layer = Dense(512, activation='relu')(tf_layer_input)\n",
        "\n",
        "# TFiDF input for extra features\n",
        "num_unique_terms = len(unique_terms)\n",
        "tfidf_layer_input = Input(shape=(num_unique_terms,))\n",
        "dense_tfidf_layer = Dense(512, activation='relu')(tfidf_layer_input)\n",
        "\n",
        "# Dictionary of terms layer\n",
        "doc_term_dic_length = 4 * maximum_length\n",
        "term_dict_layer_input = Input(shape=(doc_term_dic_length,))\n",
        "dense_term_dict_layer = Dense(512, activation='relu')(term_dict_layer_input)\n",
        "\n",
        "# # pairs input for extra features\n",
        "pair_layer_input = Input(shape=(len(selected_pairs),))\n",
        "dense_pair_layer = Dense(512, activation='relu')(pair_layer_input)\n",
        "\n",
        "# pairs input for extra features\n",
        "# pair_3_layer_input = Input(shape=(len(selected_pairs_3),))\n",
        "# dense_pair_3_layer = Dense(512, activation='relu')(pair_3_layer_input)\n",
        "\n",
        "# # pair terms (tuples size 2) text input\n",
        "# text_pairs_input = Input(shape=(None,2))\n",
        "# text_pairs_flatten_layer = Flatten()(text_pairs_input)\n",
        "# dense_text_pairs_layer = Dense(256, activation='relu')(text_pairs_flatten_layer)\n",
        "\n",
        "# Merge the outputs of the main text input and auxiliary input\n",
        "merged = concatenate([lstm_layer, dense_pos_layer, dense_pair_layer])\n",
        "\n",
        "# Additional layers for further processing\n",
        "dense_layer_1 = Dense(128, activation='relu')(merged)\n",
        "dropout_layer_1 = Dropout(0.2)(dense_layer_1)\n",
        "dense_layer_2 = Dense(64, activation='relu')(dropout_layer_1)\n",
        "dropout_layer_2 = Dropout(0.2)(dense_layer_2)\n",
        "dense_layer_3 = Dense(32, activation='relu')(dropout_layer_2)\n",
        "output_layer = Dense(num_classes, activation='softmax')(dense_layer_3)\n",
        "\n",
        "\n",
        "# Define the model\n",
        "model = Model(inputs=[text_input, pos_layer_input, pair_layer_input], outputs=output_layer)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "# model.fit({'text_input': trainDocs['token_padded'], 'extra_input': trainDocs['pos_padded']},\n",
        "#           {'output': trainTopics['one_hot']},\n",
        "#           epochs=10, batch_size=32, validation_split=0.2)\n",
        "model.fit([trainDocs_X, trainPos_X, trainPairs_X], train_Y, epochs=5, batch_size=64, validation_data=([valDocs_X, valPos_X, valPairs_X], val_Y))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vwg3xjJ0Z6g4"
      },
      "outputs": [],
      "source": [
        "inputs_list = [text_input, pos_layer_input, tf_layer_input, tfidf_layer_input, term_dict_layer_input, pair_layer_input]\n",
        "dense_layers_list = [lstm_layer, dense_pos_layer, dense_tf_layer, dense_tfidf_layer, dense_term_dict_layer, dense_pair_layer]\n",
        "train_features_list = [trainDocs_X, trainPos_X, trainTf_X, trainTfidf_X, trainTermDict_X, trainPairs_X]\n",
        "validation_features_list = [valDocs_X, valPos_X, valTf_X, valTfidf_X, valTermDict_X, valPairs_X]\n",
        "test_features_list = [testDocs_X, testPos_X, testTf_X, testTfidf_X, testTermDict_X, testPairs_X]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dV3-TL9x8cJ"
      },
      "source": [
        "# **Evaluate Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLNQX_vFvTnk"
      },
      "source": [
        "**Validation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rH3t7Ao4V5KX",
        "outputId": "d363271a-7469-4654-d614-2b60bc9dafc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 12s 1s/step\n",
            "F1 Score: [0.89719626 0.4        0.72131148 0.94285714]\n",
            "Precision: [0.84210526 0.75       0.6875     0.97058824]\n",
            "Recall: [0.96       0.27272727 0.75862069 0.91666667]\n"
          ]
        }
      ],
      "source": [
        "# val_predictions = model.predict([valTfidf_X, valTermDict_X, valPairs_X])\n",
        "val_predictions = model.predict([valDocs_X, valPos_X, valPairs_X])\n",
        "\n",
        "# Convert predictions and true labels to class labels\n",
        "val_true_labels = np.argmax(val_Y, axis=1)\n",
        "val_pred_labels = np.argmax(val_predictions, axis=1)\n",
        "\n",
        "# Calculate precision, recall, and F1 score\n",
        "precision = precision_score(val_true_labels, val_pred_labels, average=None)\n",
        "recall = recall_score(val_true_labels, val_pred_labels, average=None)\n",
        "f1 = f1_score(val_true_labels, val_pred_labels, average=None)\n",
        "\n",
        "print(f'F1 Score: {f1}')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imA9p0XpvXGM"
      },
      "source": [
        "**Test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lK2bR1zx7Fwu",
        "outputId": "fe09450b-d54c-4bdb-dca5-77d3c686d04e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "13/13 [==============================] - 25s 2s/step - loss: 0.7408 - accuracy: 0.8875\n",
            "13/13 [==============================] - 11s 812ms/step\n",
            "Test Loss: 0.7407768368721008\n",
            "Test Accuracy: 0.887499988079071\n",
            "F1-score per class: [0.8641115  0.41935484 0.53225806 0.95111111]\n",
            "Precision per class: [0.77987421 0.26530612 0.38372093 0.97272727]\n",
            "Recall per class: [0.96875    1.         0.86842105 0.93043478]\n",
            "Micro F-score: 0.8320693391115928\n",
            "Macro F-score: 0.6917088781486893\n"
          ]
        }
      ],
      "source": [
        "evaluation_results = model.evaluate([testDocs_X, testPos_X, testPairs_X], test_y)\n",
        "predictions = pd.DataFrame(model.predict([testDocs_X, testPos_X, testPairs_X]))\n",
        "# predictions = pd.DataFrame(model.predict([testPos_X, testTf_X, testTfidf_X, testTermDict_X, testPairs_X]))\n",
        "predictions['classes_probs'] = predictions.apply(lambda row: list(row), axis=1)\n",
        "# predictions['pred_label'] = predictions['classes_probs'].apply(lambda lst: np.argmax(lst))\n",
        "predictions['pred_label'] = predictions['classes_probs'].apply(lambda lst: (np.array(lst) >= 0.05).astype(int))\n",
        "\n",
        "predicted_y = np.array(predictions['pred_label'].tolist())\n",
        "\n",
        "precision_per_class = precision_score(test_y, predicted_y, average=None)\n",
        "recall_per_class = recall_score(test_y, predicted_y, average=None)\n",
        "f1score_per_class = f1_score(test_y, predicted_y, average=None)\n",
        "micro_f_score = f1_score(test_y, predicted_y, average='micro')\n",
        "macro_f_score = f1_score(test_y, predicted_y, average='macro')\n",
        "\n",
        "\n",
        "print(\"Test Loss:\", evaluation_results[0])\n",
        "print(\"Test Accuracy:\", evaluation_results[1])\n",
        "print(\"F1-score per class:\", f1score_per_class)\n",
        "print(\"Precision per class:\", precision_per_class)\n",
        "print(\"Recall per class:\", recall_per_class)\n",
        "print(\"Micro F-score:\", micro_f_score)\n",
        "print(\"Macro F-score:\", macro_f_score)\n",
        "\n",
        "# confusion_mat = confusion_matrix(test_y, predicted_y)\n",
        "# print(\"Confusion Matrix:\")\n",
        "# print(confusion_mat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "It_1dzcQtUpK"
      },
      "source": [
        "# **Save Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQPQIcjHgSFI"
      },
      "outputs": [],
      "source": [
        "model.save('NN_Ver3_Exp3.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyqVnE0xtb1m"
      },
      "source": [
        "# **Confussion Matrix**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zwgv3FjCeYH"
      },
      "outputs": [],
      "source": [
        "for i in range(test_y.shape[1]):\n",
        "    precision_per_class = precision_score(test_y[:, i], predicted_y[:, i], average=None)\n",
        "    recall_per_class = recall_score(test_y[:, i], predicted_y[:, i], average=None)\n",
        "\n",
        "    print(\"Precision per class:\", precision_per_class)\n",
        "    print(\"Recall per class:\", recall_per_class)\n",
        "\n",
        "    cm = confusion_matrix(test_y[:, i], predicted_y[:, i])\n",
        "    print(f\"Confusion Matrix for label {i}:\")\n",
        "    print(cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnq_kq-iAQ-E"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# F-scores for 6 experiments\n",
        "f_scores = [0.414, 0.601, 0.72, 0.194, 0.215, 0.3]\n",
        "experiments = ['Exp1', 'Exp2', 'Exp3', 'Exp4', 'Exp5', 'Exp6']\n",
        "\n",
        "# Create a bar plot\n",
        "fig, ax = plt.subplots()\n",
        "bars = ax.bar(experiments, f_scores, color=['blue', 'green', 'red', 'purple', 'orange', 'cyan'])\n",
        "\n",
        "# Add labels and title\n",
        "ax.set_xlabel('Experiment')\n",
        "ax.set_ylabel('F-score')\n",
        "ax.set_title('F-scores of Different Experiments')\n",
        "\n",
        "# Create a legend\n",
        "legend_labels = [f'Experiment {i+1}' for i in range(len(experiments))]\n",
        "ax.legend(bars, legend_labels)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSz8-_lvTMFx"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# F-scores for 6 experiments (main columns)\n",
        "f_scores = values = [0.39, 0.38, 0.68, 0.49, 0.62, 0.55]\n",
        "\n",
        "experiments = ['Exp1', 'Exp2', 'Exp3', 'Exp4', 'Exp5', 'Exp6']\n",
        "\n",
        "# Sub-columns data for each experiment\n",
        "sub_scores = [\n",
        "    [0.51, 0.1, 0.26, 0.70],\n",
        "    [0.54, 0.0, 0.26, 0.72],\n",
        "    [0.79, 0.55, 0.47, 0.92],\n",
        "    [0.64, 0.0, 0.5, 0.80],\n",
        "    [0.70, 0.47, 0.37, 0.93],\n",
        "    [0.71, 0.47, 0.27, 0.77],\n",
        "]\n",
        "\n",
        "# Define width of bars\n",
        "bar_width = 0.1\n",
        "\n",
        "# Define positions for bars\n",
        "main_bar_positions = np.arange(len(experiments))\n",
        "sub_bar_positions = [main_bar_positions + bar_width * (i + 1) for i in range(4)]\n",
        "\n",
        "# Create the plot\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Plot main columns (background columns)\n",
        "main_bars = ax.bar(main_bar_positions, f_scores, color='grey', width=bar_width*4, label='Macro F-score')\n",
        "\n",
        "my_list = np.array([0, 1, 2, 3])\n",
        "\n",
        "# Plot sub-columns\n",
        "for i, sub_score in enumerate(sub_scores):\n",
        "    if i == 0:\n",
        "        sub_bars = ax.bar(main_bar_positions[i] + (bar_width * my_list), sub_score, width=bar_width, label=['acq', 'corn', 'crude', 'earn'], color=['green', 'red', 'purple', 'orange'])\n",
        "    else:\n",
        "        sub_bars = ax.bar(main_bar_positions[i] + (bar_width * my_list), sub_score, width=bar_width, color=['green', 'red', 'purple', 'orange'])\n",
        "\n",
        "# Add labels and title\n",
        "ax.set_xlabel('Experiment')\n",
        "ax.set_ylabel('F-score')\n",
        "ax.set_title('F-scores of Different Experiments')\n",
        "ax.set_xticks(main_bar_positions + bar_width * 1.5)\n",
        "ax.set_xticklabels(experiments)\n",
        "\n",
        "# Create a legend\n",
        "ax.legend(bbox_to_anchor=(1, 1), loc='upper left')\n",
        "\n",
        "plt.savefig('experiment_f_scores_NN4_3features.png', dpi=300, bbox_inches='tight')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlTgG2NCWHEu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtxuz3cD6UQb"
      },
      "source": [
        "# **Coherence**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXLWksQAIKmy"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "\n",
        "p_topics = np.argmax(predicted_y == 1, axis=1)\n",
        "p_topics = [[item] for item in p_topics]\n",
        "\n",
        "c_dictionary = Dictionary(testDocs['preprocess'])\n",
        "# c_dictionary\n",
        "\n",
        "c_corpus = [c_dictionary.doc2bow(doc) for doc in testDocs['preprocess']]\n",
        "# c_corpus\n",
        "\n",
        "# Create CoherenceModel\n",
        "cm = CoherenceModel(topics=p_topics, corpus=c_corpus, dictionary=c_dictionary, coherence='u_mass')\n",
        "cm.get_coherence_per_topic()\n",
        "# Get coherence value\n",
        "# coherence = cm.get_coherence()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hyCRbFbn57V"
      },
      "source": [
        "# **Perplexity**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPdQj0sqe6pH"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import log_loss\n",
        "\n",
        "pred_prob = predictions['classes_probs'].to_list()\n",
        "true_y = [[sublist[0]] for sublist in testTopics['topics_lst']]\n",
        "cross_entropy_loss = log_loss(np.array(true_y), np.array(pred_prob))\n",
        "perplexity = np.exp(cross_entropy_loss)\n",
        "perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxXu-cFQ59e8"
      },
      "outputs": [],
      "source": [
        "np.array(pred_prob)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wh9SPrk8otMV"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnQu2pGi6S08"
      },
      "outputs": [],
      "source": [
        "# word_topic = np.zeros(shape=(vocab_size, len(favorite_topics)))\n",
        "\n",
        "# for doc_index in trainDocs.index:\n",
        "#     topic_list = trainTopics.loc[doc_index]['topics_lst']\n",
        "#     token_list = trainDocs.loc[doc_index]['vectorized']\n",
        "#     for token in token_list:\n",
        "#         for topic in topic_list:\n",
        "#             word_topic[token, topic] += 1\n",
        "\n",
        "# predicted_y\n",
        "\n",
        "# testDocs['preprocess']\n",
        "\n",
        "# import gensim\n",
        "# from gensim.corpora.dictionary import Dictionary\n",
        "# from gensim.models.coherencemodel import CoherenceModel\n",
        "# c_dictionary = Dictionary(testDocs['preprocess'])\n",
        "\n",
        "# c_corpus = [c_dictionary.doc2bow(doc) for doc in testDocs['preprocess']]\n",
        "# c_corpus\n",
        "\n",
        "# Create CoherenceModel\n",
        "# cm = CoherenceModel(topics=list_of_lists, corpus=c_corpus, dictionary=c_dictionary, coherence='u_mass')\n",
        "\n",
        "# # Get coherence value\n",
        "coherence = cm.get_coherence()\n",
        "\n",
        "# a = np.argmax(predicted_y == 1, axis=1)\n",
        "# list_of_lists = [[item] for item in a]\n",
        "# list_of_lists\n",
        "coherence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdVz72S7JLPm"
      },
      "outputs": [],
      "source": [
        "list_of_lists"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyKxqcHjLuSU"
      },
      "source": [
        "# **GloVe Test Section**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSugRqEkMNJr",
        "outputId": "32bccbba-bb36-4c90-f34d-b6445b8bc8c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-06-24 09:57:18--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2024-06-24 09:57:18--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2024-06-24 09:57:18--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.01MB/s    in 2m 39s  \n",
            "\n",
            "2024-06-24 09:59:57 (5.18 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P--w6EwbMNgy"
      },
      "outputs": [],
      "source": [
        "# 2. Load GloVe embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kySb8vaqPNzl",
        "outputId": "608aa57e-60ee-43a4-d90f-d20b91e87cba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[2, 3, 4, 1]]"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer = Tokenizer(num_words=100)\n",
        "tokenizer.fit_on_texts(['My name is Ali Hossein new all you can do call me john PC security. I love Ali'])\n",
        "tokenizer.texts_to_sequences(['My name is Ali'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSQi0Lm71XV-",
        "outputId": "f949064b-ca0b-45e9-d504-270ba73e6e5e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ali': 1,\n",
              " 'my': 2,\n",
              " 'name': 3,\n",
              " 'is': 4,\n",
              " 'hossein': 5,\n",
              " 'new': 6,\n",
              " 'all': 7,\n",
              " 'you': 8,\n",
              " 'can': 9,\n",
              " 'do': 10,\n",
              " 'call': 11,\n",
              " 'me': 12,\n",
              " 'john': 13,\n",
              " 'pc': 14,\n",
              " 'security': 15,\n",
              " 'i': 16,\n",
              " 'love': 17}"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rusOcd7GH3PW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNVbw5ZyJurK",
        "outputId": "cadd1a88-44c2-47d2-dc59-868dca7b6955"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(7503, 100)"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embedding_matrix.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "BLquA9meLtgc",
        "outputId": "1cc81fc5-b986-4985-d61b-c983bbe8dfd7"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'num_unique_terms' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-ce36504e3e19>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# TF input for extra features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtf_layer_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_unique_terms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mdense_tf_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_layer_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'num_unique_terms' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "text_input = Input(shape=(maximum_length,))\n",
        "embedding_layer = Embedding(input_dim=vocab_size,\n",
        "                            output_dim=embedding_dim,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=maximum_length,\n",
        "                            trainable=False)(text_input)\n",
        "lstm_layer = LSTM(units=512)(embedding_layer)\n",
        "\n",
        "# POS tags input for extra features\n",
        "pos_layer_input = Input(shape=(num_extra_features,))\n",
        "dense_pos_layer = Dense(512, activation='relu')(pos_layer_input)\n",
        "\n",
        "# TF input for extra features\n",
        "tf_layer_input = Input(shape=(num_unique_terms,))\n",
        "dense_tf_layer = Dense(512, activation='relu')(tf_layer_input)\n",
        "\n",
        "# TFiDF input for extra features\n",
        "tfidf_layer_input = Input(shape=(num_unique_terms,))\n",
        "dense_tfidf_layer = Dense(512, activation='relu')(tfidf_layer_input)\n",
        "\n",
        "# Dictionary of terms layer\n",
        "term_dict_layer_input = Input(shape=(doc_term_dic_length,))\n",
        "dense_term_dict_layer = Dense(512, activation='relu')(term_dict_layer_input)\n",
        "\n",
        "# Pairs input for extra features\n",
        "pair_layer_input = Input(shape=(len(selected_pairs),))\n",
        "dense_pair_layer = Dense(512, activation='relu')(pair_layer_input)\n",
        "\n",
        "# Merge the outputs of the main text input and auxiliary input\n",
        "merged = concatenate([lstm_layer, dense_pos_layer, dense_tf_layer, dense_tfidf_layer, dense_term_dict_layer, dense_pair_layer])\n",
        "\n",
        "# Additional layers for further processing\n",
        "dense_layer_1 = Dense(128, activation='relu')(merged)\n",
        "dense_layer_2 = Dense(64, activation='relu')(dense_layer_1)\n",
        "dense_layer_3 = Dense(32, activation='relu')(dense_layer_2)\n",
        "output_layer = Dense(num_classes, activation='softmax')(dense_layer_3)\n",
        "\n",
        "# Define the model\n",
        "model = Model(inputs=[text_input, pos_layer_input, tf_layer_input, tfidf_layer_input, term_dict_layer_input, pair_layer_input], outputs=output_layer)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit([trainDocs_X, trainPos_X, trainTf_X, trainTfidf_X, trainTermDict_X, trainPairs_X], train_Y, epochs=10, batch_size=32, validation_data=([valDocs_X, valPos_X, valTf_X, valTfidf_X, valTermDict_X, valPairs_X], val_Y))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "DPlHTYA05qWL",
        "MGSA5l1mN2I7",
        "h0sbrqiZyqre",
        "gyDYe-MKyrOA",
        "QmcFLPx8U_0e",
        "vBn_8r789YT3",
        "TAiG1vSQ-GlM",
        "rCxMcty3_XKr",
        "Kw8obdfhLNXt",
        "Dp_AzMU4LOkl",
        "757dPthIOnqu",
        "ALHuYuZN14eM",
        "n-GKyjGnjbaF",
        "YuGxOq79joCs",
        "iklnNbA8jomV",
        "g2plhochKzbC",
        "_S5rj6lZLeVQ",
        "RLbh5gnGVYg7",
        "OrWRE9vL3HJS",
        "gGtzwM1N9lsf",
        "-gRGdnG66yn3",
        "ecwBbTNOSpU4",
        "4xq4dSl5xV_x",
        "-JlshDBtWeS8",
        "1mTgq1LFo4VI",
        "3McFDJP3-RLL",
        "TS8FntbK_tEE",
        "BKzCk13OU5f4",
        "o2uBDhkMwtti",
        "j4SsXrx3m8GC"
      ],
      "provenance": [],
      "mount_file_id": "1jaMTsjpVepm_5tEp-o-hM3wo7Rr4IrDk",
      "authorship_tag": "ABX9TyP/FHtI3Oejr0XC2Ppb55To"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}